{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-Rola/Advanced-Machine-Learning-2022-2023/blob/main/Project_Federated_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9QcGnGPdX2C"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k9O3aM3Tb28q",
        "outputId": "cd324c81-8223-40e5-9e8e-3e6787dce449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.3.1 (from versions: 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.3.1\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchvision==0.5.0\n",
            "  Downloading torchvision-0.5.0-cp38-cp38-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 26.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (1.21.6)\n",
            "Collecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp38-cp38-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow-SIMD\n",
            "  Downloading Pillow-SIMD-9.0.0.post1.tar.gz (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 36.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-9.0.0.post1-cp38-cp38-linux_x86_64.whl size=1215581 sha256=2f5478fe3bb641584182b6a440556485971ce0424253b136d3ce26d43b940743\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/43/70/7bfd3ad17119c79fb9455093978fda4d5eb8f750ad6a0db259\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-9.0.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Collecting pillow\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 20.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip install -U pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo942LMOdlh4"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DokFOdD1dJEl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from statistics import mean \n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet, vgg16, resnet18, resnet50\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIDLJuIXK_vh"
      },
      "source": [
        "**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5PkYfqfK_SA"
      },
      "outputs": [],
      "source": [
        "\n",
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 100    \n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.1      # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 0.0001  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 160   # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 1         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 25\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwii0TBHvzh"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUDdw4j2H0Mc"
      },
      "outputs": [],
      "source": [
        "PRE_TRAINED = False     # set to True to load the pre-trained AlexNet\n",
        "\n",
        "NETWORK_TYPE = \"resnet20\"       #define which network we will use:\n",
        "                              #alexnet, vgg, resnet\n",
        "\n",
        "FREEZING = \"no_freezing\"        # define which layers of the network will be kept frozen\n",
        "                                # None : train the whole network\n",
        "                                # \"CONV\" : train only the FC-layers\n",
        "                                # \"FC\" : train only the conv-layers\n",
        "\n",
        "AUG_PROB = 0.5   # the probability with witch each image is transformed at training time during each epoch\n",
        "AUG_TYPE = \"RC-RHF\"         # define the type of augmentation pipeline \n",
        "                            # None for no data augmentation\n",
        "                            # \"CS-HF\" for contrast + saturation + horizontal flip\n",
        "                            # \"H-RP\" for hue + random perspective\n",
        "                            # \"B-GS-R\" for brightness + grayscale + rotation\n",
        "                            # \"RC-RHF\" random crop + random horizontal flip => for the project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lokd8CQSdXI4"
      },
      "source": [
        "**Trasformations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLlB-7B-Cg1A"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "if PRE_TRAINED:\n",
        "  normalizer = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "else:\n",
        "  normalizer = transforms.Normalize(mean = (0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))\n",
        "\n",
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "bright_t = transforms.ColorJitter(brightness=[1,2])\n",
        "contrast_t = transforms.ColorJitter(contrast = [2,5])\n",
        "saturation_t = transforms.ColorJitter(saturation = [1,3])\n",
        "hue_t = transforms.ColorJitter(hue = 0.2)\n",
        "gs_t = transforms.Grayscale(3)\n",
        "rp_t = transforms.RandomPerspective(p = 1, distortion_scale = 0.5)\n",
        "rot_t = transforms.RandomRotation(degrees = 90)\n",
        "rand_crop = transforms.RandomCrop(32, padding = 4)\n",
        "hflip_t = transforms.RandomHorizontalFlip(p = 1)\n",
        "\n",
        "aug_transformations = {\n",
        "    \"CS-HF\": transforms.Compose([contrast_t, saturation_t, hflip_t]),\n",
        "    \"H-RP\": transforms.Compose([hue_t, rp_t]),\n",
        "    \"B-GS-R\": transforms.Compose([bright_t, gs_t, rot_t]),\n",
        "    \"RC-RHF\": transforms.Compose([rand_crop, hflip_t])\n",
        "    }\n",
        "\n",
        "if AUG_TYPE is not None:\n",
        "  aug_transformation = aug_transformations[AUG_TYPE]\n",
        "  aug_pipeline = transforms.Compose([ \n",
        "                                      transforms.ToPILImage(),\n",
        "                                      transforms.RandomApply([aug_transformation], p = AUG_PROB),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalizer\n",
        "                                     ])\n",
        "else:\n",
        "  aug_pipeline = normalizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKBuFxHvdi3l"
      },
      "source": [
        "**Define Training set, validation set and testing**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "3430fd8b8f3647ee8c90a217b2b0dc1b",
            "c6dcd503460e49cba8eb1304ab1c71c0",
            "61e57d90bd1a4c2d9181eabf74406c00",
            "ec3bc76d5f1949c6a4a6f7ef051925ce",
            "27e83afbd28145238db1781af8671328",
            "9a4939aa420a4166b65fb9548d90fe30",
            "1fd734c10c4e4de0858e6332d2426e04",
            "49c8d9f40565447097e5c93f043e3cad",
            "b93241990a674a389489c5deddfcf77f",
            "2ede10f96f5540849e8a268d94c273b3",
            "e1ff9c5deca94aa199ba4528892cf84f"
          ]
        },
        "id": "aoNyzNmEdhHp",
        "outputId": "c9cbc87e-10e3-4414-b5c6-dc9a3af687a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3430fd8b8f3647ee8c90a217b2b0dc1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "{'cattle': 500, 'dinosaur': 500, 'apple': 500, 'boy': 500, 'aquarium_fish': 500, 'telephone': 500, 'train': 500, 'cup': 500, 'cloud': 500, 'elephant': 500, 'keyboard': 500, 'willow_tree': 500, 'sunflower': 500, 'castle': 500, 'sea': 500, 'bicycle': 500, 'wolf': 500, 'squirrel': 500, 'shrew': 500, 'pine_tree': 500, 'rose': 500, 'television': 500, 'table': 500, 'possum': 500, 'oak_tree': 500, 'leopard': 500, 'maple_tree': 500, 'rabbit': 500, 'chimpanzee': 500, 'clock': 500, 'streetcar': 500, 'cockroach': 500, 'snake': 500, 'lobster': 500, 'mountain': 500, 'palm_tree': 500, 'skyscraper': 500, 'tractor': 500, 'shark': 500, 'butterfly': 500, 'bottle': 500, 'bee': 500, 'chair': 500, 'woman': 500, 'hamster': 500, 'otter': 500, 'seal': 500, 'lion': 500, 'mushroom': 500, 'girl': 500, 'sweet_pepper': 500, 'forest': 500, 'crocodile': 500, 'orange': 500, 'tulip': 500, 'mouse': 500, 'camel': 500, 'caterpillar': 500, 'man': 500, 'skunk': 500, 'kangaroo': 500, 'raccoon': 500, 'snail': 500, 'rocket': 500, 'whale': 500, 'worm': 500, 'turtle': 500, 'beaver': 500, 'plate': 500, 'wardrobe': 500, 'road': 500, 'fox': 500, 'flatfish': 500, 'tiger': 500, 'ray': 500, 'dolphin': 500, 'poppy': 500, 'porcupine': 500, 'lamp': 500, 'crab': 500, 'motorcycle': 500, 'spider': 500, 'tank': 500, 'orchid': 500, 'lizard': 500, 'beetle': 500, 'bridge': 500, 'baby': 500, 'lawn_mower': 500, 'house': 500, 'bus': 500, 'couch': 500, 'bowl': 500, 'pear': 500, 'bed': 500, 'plain': 500, 'trout': 500, 'bear': 500, 'pickup_truck': 500, 'can': 500}\n",
            "classi totali: 100\n",
            "Train Dataset: 45000\n",
            "Valid Dataset: 5000\n",
            "Test Dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "classes = trainset.classes\n",
        "class_count = {}\n",
        "for _, index in trainset:\n",
        "    label = classes[index]\n",
        "    if label not in class_count:\n",
        "        class_count[label] = 0\n",
        "    class_count[label] += 1\n",
        "print(class_count)\n",
        "print(f\"classi totali: {len(class_count)}\")\n",
        "\n",
        "val_size = 5000\n",
        "train_size = len(trainset) - val_size\n",
        "trainset, validset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2, drop_last=True)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(trainset)))\n",
        "print('Valid Dataset: {}'.format(len(validset)))\n",
        "print('Test Dataset: {}'.format(len(testset)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzbqaiCd5Ut"
      },
      "source": [
        "**Show immages and classes of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "fQOaKspruOKd",
        "outputId": "9e54432c-a1a7-49ff-a7af-8c16c3506193"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "snail lobster cup   trout bowl  mouse house snake orchid pine_tree porcupine elephant sunflower sweet_pepper mountain train lizard rose  otter clock plate train shrew couch rocket bus   clock lobster streetcar lamp  otter kangaroo chair squirrel orange cattle squirrel apple road  forest mouse bowl  willow_tree cockroach fox   snake telephone tank  telephone bus   raccoon crocodile willow_tree orchid camel cloud mouse kangaroo crocodile lamp  television lobster clock mountain elephant shrew television cloud wardrobe road  man   willow_tree bridge rabbit streetcar bridge aquarium_fish apple train ray   sweet_pepper fox   turtle clock house oak_tree turtle shrew beaver worm  tractor lamp  tank  tractor pear  lamp  streetcar chair kangaroo worm  chair raccoon lizard cup   lamp  motorcycle wolf  kangaroo couch bus   house girl  chair pine_tree bowl  cattle leopard otter skunk bowl  aquarium_fish palm_tree spider tiger trout chair mouse squirrel\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image (maybe to change the unnormalize part)\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucLk-IqleGrL"
      },
      "source": [
        "**Build resNet20**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMzOLkdtYwXM"
      },
      "outputs": [],
      "source": [
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='B'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        #self.bn1 = nn.BatchNorm2d(planes)\n",
        "        # GroupNorm takes number of groups to divide the\n",
        "        # channels in and the number of channels to expect\n",
        "        # in the input\n",
        "        self.bn1 = nn.GroupNorm(4, planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        #self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.bn2 = nn.GroupNorm(4, planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                  nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                  #nn.BatchNorm2d(self.expansion * planes)\n",
        "                  nn.GroupNorm(4, self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        #self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn1 = nn.GroupNorm(4, 16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwlRZLXcAKLN"
      },
      "source": [
        "**Prepare Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReThsm-2AM2P",
        "outputId": "17e2c92d-90f9-4613-c615-e115e11f77b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet20\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): GroupNorm(4, 64, eps=1e-05, affine=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=64, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if NETWORK_TYPE == 'alexnet':\n",
        "  net = alexnet(pretrained=PRE_TRAINED) # Loading AlexNet model, if pretrained = True, returns a model pre-trained on ImageNet\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "# OTHER NETWORKS ------------------------------------------------------------------------------------------\n",
        "elif NETWORK_TYPE == 'vgg':\n",
        "  net = vgg16(pretrained='imagenet')\n",
        "  net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
        "\n",
        "elif NETWORK_TYPE == 'resnet20':  \n",
        "  print(\"resnet20\")\n",
        "  net = resnet20()  \n",
        "  print(net) #show details of resnet20\n",
        "\n",
        "elif NETWORK_TYPE == 'resnet18':  \n",
        "  net = resnet18()  \n",
        "  print(net) #show details of resnet18\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEyL3H_R4qCf"
      },
      "source": [
        "**Prepare Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sjq00G94tSc"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "\n",
        "# Choose parameters to optimize and which one to freeze\n",
        "if (FREEZING == 'no_freezing'):\n",
        "  parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "elif (FREEZING == 'conv_layers'):\n",
        "  parameters_to_optimize = net.classifier.parameters() # Updates only fully-connected layers (no conv)\n",
        "elif (FREEZING == 'fc_layers'):\n",
        "  parameters_to_optimize = net.features.parameters() # Updates only conv layers (no fc)\n",
        "else :\n",
        "  raise (ValueError(f\"Error Freezing layers (FREEZE = {FREEZING}) \\n Possible values are: 'no_freezing', 'conv_layers', 'fc_layers' \"))\n",
        "\n",
        "# Choose parameters to optimize\n",
        "# To access a different set of parameters, you have to access submodules of AlexNet\n",
        "# (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "# e.g.: parameters of the convolutional layers: look at alexnet's source code ;) \n",
        "#parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
        "\n",
        "# Define optimizer\n",
        "# An optimizer updates the weights based on loss\n",
        "# We use SGD with momentum\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Define scheduler\n",
        "# A scheduler dynamically changes learning rate\n",
        "# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, STEP_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41SZww8sG9Pq"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2axHPzpG7GP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(net, dataloader, print_tqdm = True):\n",
        "  with torch.no_grad():\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "    running_corrects = 0\n",
        "    iterable = tqdm(dataloader) if print_tqdm else dataloader\n",
        "    losses = []\n",
        "    for images, labels in iterable: \n",
        "      norm_images = []\n",
        "      for image in images:\n",
        "        norm_image = normalizer(image)\n",
        "        norm_images.append(norm_image)\n",
        "      norm_images = torch.stack(norm_images)  \n",
        "      norm_images = norm_images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      # Forward Pass\n",
        "      outputs = net(norm_images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "\n",
        "  return accuracy, mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxYUli9d9uYQ"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcoQ5fD49yT_",
        "outputId": "08f4732d-930b-4440-8bf5-411fd666ebf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/160, LR = [0.1]\n",
            "Step 0, Loss 0.9887531399726868\n",
            "Step 25, Loss 0.9022598266601562\n",
            "Step 50, Loss 1.055271863937378\n",
            "Step 75, Loss 0.9860667586326599\n",
            "Step 100, Loss 1.1943007707595825\n",
            "Step 125, Loss 1.0141534805297852\n",
            "Step 150, Loss 0.9811398386955261\n",
            "Step 175, Loss 0.8951401114463806\n",
            "Step 200, Loss 1.1222681999206543\n",
            "Step 225, Loss 1.0898932218551636\n",
            "Step 250, Loss 1.3591827154159546\n",
            "Step 275, Loss 1.333266019821167\n",
            "Step 300, Loss 0.9941538572311401\n",
            "Step 325, Loss 0.9252023100852966\n",
            "Step 350, Loss 1.1796408891677856\n",
            "Starting epoch 2/160, LR = [0.0993844170297569]\n",
            "Step 375, Loss 1.1976534128189087\n",
            "Step 400, Loss 1.2758641242980957\n",
            "Step 425, Loss 0.86395263671875\n",
            "Step 450, Loss 0.9280703067779541\n",
            "Step 475, Loss 1.0091241598129272\n",
            "Step 500, Loss 1.0004760026931763\n",
            "Step 525, Loss 0.9380245208740234\n",
            "Step 550, Loss 1.1130343675613403\n",
            "Step 575, Loss 1.1612790822982788\n",
            "Step 600, Loss 0.8905528783798218\n",
            "Step 625, Loss 0.8761699199676514\n",
            "Step 650, Loss 1.1740734577178955\n",
            "Step 675, Loss 1.1818854808807373\n",
            "Step 700, Loss 0.996273934841156\n",
            "Starting epoch 3/160, LR = [0.09755282581475769]\n",
            "Step 725, Loss 1.0436726808547974\n",
            "Step 750, Loss 0.8488878011703491\n",
            "Step 775, Loss 1.0453221797943115\n",
            "Step 800, Loss 0.8188087940216064\n",
            "Step 825, Loss 1.0389552116394043\n",
            "Step 850, Loss 0.9151765704154968\n",
            "Step 875, Loss 1.098161220550537\n",
            "Step 900, Loss 1.1110336780548096\n",
            "Step 925, Loss 1.0234800577163696\n",
            "Step 950, Loss 0.9911759495735168\n",
            "Step 975, Loss 0.9929726719856262\n",
            "Step 1000, Loss 1.0419583320617676\n",
            "Step 1025, Loss 1.0518637895584106\n",
            "Step 1050, Loss 1.0078473091125488\n",
            "Starting epoch 4/160, LR = [0.0945503262094184]\n",
            "Step 1075, Loss 1.2259767055511475\n",
            "Step 1100, Loss 0.8675591349601746\n",
            "Step 1125, Loss 0.7848155498504639\n",
            "Step 1150, Loss 0.9188207387924194\n",
            "Step 1175, Loss 0.9001859426498413\n",
            "Step 1200, Loss 0.8362348675727844\n",
            "Step 1225, Loss 0.8801665902137756\n",
            "Step 1250, Loss 0.9690840840339661\n",
            "Step 1275, Loss 1.0654047727584839\n",
            "Step 1300, Loss 1.1849572658538818\n",
            "Step 1325, Loss 1.0588469505310059\n",
            "Step 1350, Loss 1.0535595417022705\n",
            "Step 1375, Loss 0.9257550239562988\n",
            "Step 1400, Loss 1.0397117137908936\n",
            "Starting epoch 5/160, LR = [0.0904508497187474]\n",
            "Step 1425, Loss 0.9954363703727722\n",
            "Step 1450, Loss 1.109349250793457\n",
            "Step 1475, Loss 0.9766069650650024\n",
            "Step 1500, Loss 1.0763813257217407\n",
            "Step 1525, Loss 0.9902118444442749\n",
            "Step 1550, Loss 0.9866681694984436\n",
            "Step 1575, Loss 0.815786600112915\n",
            "Step 1600, Loss 1.1209100484848022\n",
            "Step 1625, Loss 1.291107177734375\n",
            "Step 1650, Loss 1.1410858631134033\n",
            "Step 1675, Loss 0.9567712545394897\n",
            "Step 1700, Loss 1.1371192932128906\n",
            "Step 1725, Loss 0.8836504817008972\n",
            "Step 1750, Loss 0.864162027835846\n",
            "Starting epoch 6/160, LR = [0.08535533905932739]\n",
            "Step 1775, Loss 0.8418757915496826\n",
            "Step 1800, Loss 0.9200149178504944\n",
            "Step 1825, Loss 0.7619592547416687\n",
            "Step 1850, Loss 0.9303879141807556\n",
            "Step 1875, Loss 1.0910539627075195\n",
            "Step 1900, Loss 0.7770689725875854\n",
            "Step 1925, Loss 0.8862690329551697\n",
            "Step 1950, Loss 0.8251504302024841\n",
            "Step 1975, Loss 0.924663782119751\n",
            "Step 2000, Loss 1.1394364833831787\n",
            "Step 2025, Loss 0.7256456613540649\n",
            "Step 2050, Loss 1.0652499198913574\n",
            "Step 2075, Loss 0.9709977507591248\n",
            "Step 2100, Loss 1.1211951971054077\n",
            "Starting epoch 7/160, LR = [0.07938926261462367]\n",
            "Step 2125, Loss 0.862358808517456\n",
            "Step 2150, Loss 0.9034675359725952\n",
            "Step 2175, Loss 1.066454291343689\n",
            "Step 2200, Loss 0.8693748712539673\n",
            "Step 2225, Loss 1.0033291578292847\n",
            "Step 2250, Loss 0.823844313621521\n",
            "Step 2275, Loss 1.0969430208206177\n",
            "Step 2300, Loss 0.9488464593887329\n",
            "Step 2325, Loss 0.9653201103210449\n",
            "Step 2350, Loss 0.9717265963554382\n",
            "Step 2375, Loss 1.104082465171814\n",
            "Step 2400, Loss 1.0210844278335571\n",
            "Step 2425, Loss 1.037362813949585\n",
            "Step 2450, Loss 0.7467033267021179\n",
            "Starting epoch 8/160, LR = [0.07269952498697735]\n",
            "Step 2475, Loss 0.7504518032073975\n",
            "Step 2500, Loss 0.6476580500602722\n",
            "Step 2525, Loss 0.8814346194267273\n",
            "Step 2550, Loss 0.845244288444519\n",
            "Step 2575, Loss 1.081620693206787\n",
            "Step 2600, Loss 0.9690196514129639\n",
            "Step 2625, Loss 0.8490236401557922\n",
            "Step 2650, Loss 1.0001442432403564\n",
            "Step 2675, Loss 0.8953551650047302\n",
            "Step 2700, Loss 0.8844489455223083\n",
            "Step 2725, Loss 1.0794461965560913\n",
            "Step 2750, Loss 0.851391077041626\n",
            "Step 2775, Loss 0.842494547367096\n",
            "Step 2800, Loss 0.9755282998085022\n",
            "Starting epoch 9/160, LR = [0.06545084971874739]\n",
            "Step 2825, Loss 0.7593929767608643\n",
            "Step 2850, Loss 0.874555766582489\n",
            "Step 2875, Loss 0.8922974467277527\n",
            "Step 2900, Loss 0.9521692395210266\n",
            "Step 2925, Loss 0.8555871844291687\n",
            "Step 2950, Loss 0.7011579275131226\n",
            "Step 2975, Loss 0.8943373560905457\n",
            "Step 3000, Loss 0.6485911011695862\n",
            "Step 3025, Loss 0.68154376745224\n",
            "Step 3050, Loss 0.9709029197692871\n",
            "Step 3075, Loss 0.7719113230705261\n",
            "Step 3100, Loss 1.0782917737960815\n",
            "Step 3125, Loss 0.7678708434104919\n",
            "Step 3150, Loss 0.8021925687789917\n",
            "Starting epoch 10/160, LR = [0.05782172325201156]\n",
            "Step 3175, Loss 0.687360405921936\n",
            "Step 3200, Loss 0.7921453714370728\n",
            "Step 3225, Loss 0.6833831071853638\n",
            "Step 3250, Loss 0.7742069959640503\n",
            "Step 3275, Loss 0.8193211555480957\n",
            "Step 3300, Loss 0.8221371173858643\n",
            "Step 3325, Loss 0.8230963945388794\n",
            "Step 3350, Loss 0.8467953205108643\n",
            "Step 3375, Loss 0.9435663819313049\n",
            "Step 3400, Loss 0.7766104340553284\n",
            "Step 3425, Loss 0.9299024343490601\n",
            "Step 3450, Loss 0.8417237997055054\n",
            "Step 3475, Loss 0.7020856738090515\n",
            "Step 3500, Loss 1.0494656562805176\n",
            "Starting epoch 11/160, LR = [0.05000000000000001]\n",
            "Step 3525, Loss 0.6496818661689758\n",
            "Step 3550, Loss 0.6086641550064087\n",
            "Step 3575, Loss 0.8683807849884033\n",
            "Step 3600, Loss 0.6588436365127563\n",
            "Step 3625, Loss 0.7826711535453796\n",
            "Step 3650, Loss 0.7745567560195923\n",
            "Step 3675, Loss 0.635086178779602\n",
            "Step 3700, Loss 0.567547619342804\n",
            "Step 3725, Loss 0.7723199725151062\n",
            "Step 3750, Loss 0.6901090145111084\n",
            "Step 3775, Loss 0.715596079826355\n",
            "Step 3800, Loss 0.622207760810852\n",
            "Step 3825, Loss 0.9807032346725464\n",
            "Step 3850, Loss 0.6554763913154602\n",
            "Starting epoch 12/160, LR = [0.04217827674798848]\n",
            "Step 3875, Loss 0.7107861638069153\n",
            "Step 3900, Loss 0.5933752059936523\n",
            "Step 3925, Loss 0.7257009744644165\n",
            "Step 3950, Loss 0.6018686294555664\n",
            "Step 3975, Loss 0.6442612409591675\n",
            "Step 4000, Loss 0.5145641565322876\n",
            "Step 4025, Loss 0.518510639667511\n",
            "Step 4050, Loss 0.6838688254356384\n",
            "Step 4075, Loss 0.5416273474693298\n",
            "Step 4100, Loss 0.6184954643249512\n",
            "Step 4125, Loss 0.710908830165863\n",
            "Step 4150, Loss 0.7198846340179443\n",
            "Step 4175, Loss 0.6148480772972107\n",
            "Step 4200, Loss 0.7321527600288391\n",
            "Starting epoch 13/160, LR = [0.03454915028125264]\n",
            "Step 4225, Loss 0.5298752188682556\n",
            "Step 4250, Loss 0.5286654233932495\n",
            "Step 4275, Loss 0.6728222966194153\n",
            "Step 4300, Loss 0.5350884199142456\n",
            "Step 4325, Loss 0.5055976510047913\n",
            "Step 4350, Loss 0.8855986595153809\n",
            "Step 4375, Loss 0.7354656457901001\n",
            "Step 4400, Loss 0.693637490272522\n",
            "Step 4425, Loss 0.6874181628227234\n",
            "Step 4450, Loss 0.6091113686561584\n",
            "Step 4475, Loss 0.5869566202163696\n",
            "Step 4500, Loss 0.5707901120185852\n",
            "Step 4525, Loss 0.7117160558700562\n",
            "Step 4550, Loss 0.7722712159156799\n",
            "Starting epoch 14/160, LR = [0.027300475013022667]\n",
            "Step 4575, Loss 0.5546712875366211\n",
            "Step 4600, Loss 0.6461487412452698\n",
            "Step 4625, Loss 0.511705756187439\n",
            "Step 4650, Loss 0.5554733276367188\n",
            "Step 4675, Loss 0.5191919207572937\n",
            "Step 4700, Loss 0.6168916821479797\n",
            "Step 4725, Loss 0.3953406512737274\n",
            "Step 4750, Loss 0.7000535726547241\n",
            "Step 4775, Loss 0.44874438643455505\n",
            "Step 4800, Loss 0.5414150357246399\n",
            "Step 4825, Loss 0.5199893116950989\n",
            "Step 4850, Loss 0.7346437573432922\n",
            "Step 4875, Loss 0.4806331992149353\n",
            "Step 4900, Loss 0.644675076007843\n",
            "Starting epoch 15/160, LR = [0.020610737385376353]\n",
            "Step 4925, Loss 0.5600166916847229\n",
            "Step 4950, Loss 0.4640735983848572\n",
            "Step 4975, Loss 0.4242706298828125\n",
            "Step 5000, Loss 0.6245827078819275\n",
            "Step 5025, Loss 0.548922061920166\n",
            "Step 5050, Loss 0.6111483573913574\n",
            "Step 5075, Loss 0.3720605969429016\n",
            "Step 5100, Loss 0.5150075554847717\n",
            "Step 5125, Loss 0.6433042287826538\n",
            "Step 5150, Loss 0.5643830299377441\n",
            "Step 5175, Loss 0.49969497323036194\n",
            "Step 5200, Loss 0.517429530620575\n",
            "Step 5225, Loss 0.6160377860069275\n",
            "Step 5250, Loss 0.5588542819023132\n",
            "Starting epoch 16/160, LR = [0.014644660940672629]\n",
            "Step 5275, Loss 0.3626149296760559\n",
            "Step 5300, Loss 0.48983773589134216\n",
            "Step 5325, Loss 0.35961419343948364\n",
            "Step 5350, Loss 0.4678746461868286\n",
            "Step 5375, Loss 0.39384615421295166\n",
            "Step 5400, Loss 0.5130810141563416\n",
            "Step 5425, Loss 0.4030831456184387\n",
            "Step 5450, Loss 0.444756418466568\n",
            "Step 5475, Loss 0.4057619869709015\n",
            "Step 5500, Loss 0.4328854978084564\n",
            "Step 5525, Loss 0.5358759760856628\n",
            "Step 5550, Loss 0.40317779779434204\n",
            "Step 5575, Loss 0.4852798283100128\n",
            "Step 5600, Loss 0.5942041873931885\n",
            "Starting epoch 17/160, LR = [0.009549150281252633]\n",
            "Step 5625, Loss 0.5734551548957825\n",
            "Step 5650, Loss 0.43825018405914307\n",
            "Step 5675, Loss 0.3421444594860077\n",
            "Step 5700, Loss 0.38721850514411926\n",
            "Step 5725, Loss 0.4867796301841736\n",
            "Step 5750, Loss 0.49805358052253723\n",
            "Step 5775, Loss 0.3478489816188812\n",
            "Step 5800, Loss 0.422299325466156\n",
            "Step 5825, Loss 0.4059790074825287\n",
            "Step 5850, Loss 0.44302135705947876\n",
            "Step 5875, Loss 0.3798118829727173\n",
            "Step 5900, Loss 0.5735825300216675\n",
            "Step 5925, Loss 0.5017209053039551\n",
            "Step 5950, Loss 0.44572538137435913\n",
            "Starting epoch 18/160, LR = [0.005449673790581611]\n",
            "Step 5975, Loss 0.4668269157409668\n",
            "Step 6000, Loss 0.44498491287231445\n",
            "Step 6025, Loss 0.4954967796802521\n",
            "Step 6050, Loss 0.44253697991371155\n",
            "Step 6075, Loss 0.3722791075706482\n",
            "Step 6100, Loss 0.3858487606048584\n",
            "Step 6125, Loss 0.3338877558708191\n",
            "Step 6150, Loss 0.39776623249053955\n",
            "Step 6175, Loss 0.2992313504219055\n",
            "Step 6200, Loss 0.3656228184700012\n",
            "Step 6225, Loss 0.40876758098602295\n",
            "Step 6250, Loss 0.46270740032196045\n",
            "Step 6275, Loss 0.4088709354400635\n",
            "Step 6300, Loss 0.4241854250431061\n",
            "Starting epoch 19/160, LR = [0.0024471741852423235]\n",
            "Step 6325, Loss 0.32252275943756104\n",
            "Step 6350, Loss 0.3307318687438965\n",
            "Step 6375, Loss 0.27403104305267334\n",
            "Step 6400, Loss 0.32105278968811035\n",
            "Step 6425, Loss 0.2470511496067047\n",
            "Step 6450, Loss 0.39004579186439514\n",
            "Step 6475, Loss 0.41201066970825195\n",
            "Step 6500, Loss 0.3466101884841919\n",
            "Step 6525, Loss 0.33025574684143066\n",
            "Step 6550, Loss 0.5309982299804688\n",
            "Step 6575, Loss 0.3246592879295349\n",
            "Step 6600, Loss 0.4730072617530823\n",
            "Step 6625, Loss 0.4695398211479187\n",
            "Step 6650, Loss 0.3049081563949585\n",
            "Starting epoch 20/160, LR = [0.0006155829702431171]\n",
            "Step 6675, Loss 0.6135324835777283\n",
            "Step 6700, Loss 0.48342379927635193\n",
            "Step 6725, Loss 0.36836230754852295\n",
            "Step 6750, Loss 0.4753679633140564\n",
            "Step 6775, Loss 0.3663069009780884\n",
            "Step 6800, Loss 0.3983963131904602\n",
            "Step 6825, Loss 0.5222901105880737\n",
            "Step 6850, Loss 0.32934239506721497\n",
            "Step 6875, Loss 0.35938602685928345\n",
            "Step 6900, Loss 0.4053595960140228\n",
            "Step 6925, Loss 0.4561322331428528\n",
            "Step 6950, Loss 0.4072360098361969\n",
            "Step 6975, Loss 0.24980181455612183\n",
            "Step 7000, Loss 0.27795660495758057\n",
            "Starting epoch 21/160, LR = [0.0]\n",
            "Step 7025, Loss 0.39897844195365906\n",
            "Step 7050, Loss 0.4218958020210266\n",
            "Step 7075, Loss 0.46356701850891113\n",
            "Step 7100, Loss 0.401420533657074\n",
            "Step 7125, Loss 0.4555976390838623\n",
            "Step 7150, Loss 0.37097692489624023\n",
            "Step 7175, Loss 0.4126816689968109\n",
            "Step 7200, Loss 0.34633803367614746\n",
            "Step 7225, Loss 0.47034183144569397\n",
            "Step 7250, Loss 0.31631559133529663\n",
            "Step 7275, Loss 0.5299031734466553\n",
            "Step 7300, Loss 0.40916872024536133\n",
            "Step 7325, Loss 0.5032011866569519\n",
            "Step 7350, Loss 0.45862385630607605\n",
            "Starting epoch 22/160, LR = [0.0006155829702431115]\n",
            "Step 7375, Loss 0.47930794954299927\n",
            "Step 7400, Loss 0.46914851665496826\n",
            "Step 7425, Loss 0.6738550662994385\n",
            "Step 7450, Loss 0.4247152805328369\n",
            "Step 7475, Loss 0.2823818325996399\n",
            "Step 7500, Loss 0.5233071446418762\n",
            "Step 7525, Loss 0.3018189072608948\n",
            "Step 7550, Loss 0.4577970504760742\n",
            "Step 7575, Loss 0.3734302222728729\n",
            "Step 7600, Loss 0.44750362634658813\n",
            "Step 7625, Loss 0.435263067483902\n",
            "Step 7650, Loss 0.33586201071739197\n",
            "Step 7675, Loss 0.5725438594818115\n",
            "Step 7700, Loss 0.3976825773715973\n",
            "Starting epoch 23/160, LR = [0.0024471741852423123]\n",
            "Step 7725, Loss 0.27021580934524536\n",
            "Step 7750, Loss 0.3899909257888794\n",
            "Step 7775, Loss 0.37096983194351196\n",
            "Step 7800, Loss 0.3032483458518982\n",
            "Step 7825, Loss 0.5122061371803284\n",
            "Step 7850, Loss 0.3505178391933441\n",
            "Step 7875, Loss 0.4042499363422394\n",
            "Step 7900, Loss 0.4300301969051361\n",
            "Step 7925, Loss 0.38846516609191895\n",
            "Step 7950, Loss 0.38654589653015137\n",
            "Step 7975, Loss 0.3070921003818512\n",
            "Step 8000, Loss 0.30288124084472656\n",
            "Step 8025, Loss 0.5078041553497314\n",
            "Step 8050, Loss 0.3215869069099426\n",
            "Starting epoch 24/160, LR = [0.005449673790581605]\n",
            "Step 8075, Loss 0.34392014145851135\n",
            "Step 8100, Loss 0.49878278374671936\n",
            "Step 8125, Loss 0.4460318088531494\n",
            "Step 8150, Loss 0.5259240865707397\n",
            "Step 8175, Loss 0.3801456689834595\n",
            "Step 8200, Loss 0.3802852928638458\n",
            "Step 8225, Loss 0.44538381695747375\n",
            "Step 8250, Loss 0.35112062096595764\n",
            "Step 8275, Loss 0.3890899419784546\n",
            "Step 8300, Loss 0.3841838836669922\n",
            "Step 8325, Loss 0.34523874521255493\n",
            "Step 8350, Loss 0.5046310424804688\n",
            "Step 8375, Loss 0.33884918689727783\n",
            "Step 8400, Loss 0.3853917419910431\n",
            "Starting epoch 25/160, LR = [0.009549150281252621]\n",
            "Step 8425, Loss 0.452261745929718\n",
            "Step 8450, Loss 0.429210901260376\n",
            "Step 8475, Loss 0.46452444791793823\n",
            "Step 8500, Loss 0.46791115403175354\n",
            "Step 8525, Loss 0.4958343803882599\n",
            "Step 8550, Loss 0.4063524603843689\n",
            "Step 8575, Loss 0.3055076599121094\n",
            "Step 8600, Loss 0.40803608298301697\n",
            "Step 8625, Loss 0.4931271970272064\n",
            "Step 8650, Loss 0.4039710462093353\n",
            "Step 8675, Loss 0.4814508557319641\n",
            "Step 8700, Loss 0.3678450584411621\n",
            "Step 8725, Loss 0.625842273235321\n",
            "Step 8750, Loss 0.395748496055603\n",
            "Starting epoch 26/160, LR = [0.014644660940672615]\n",
            "Step 8775, Loss 0.32733914256095886\n",
            "Step 8800, Loss 0.5345818996429443\n",
            "Step 8825, Loss 0.5389262437820435\n",
            "Step 8850, Loss 0.38778215646743774\n",
            "Step 8875, Loss 0.48486965894699097\n",
            "Step 8900, Loss 0.49440866708755493\n",
            "Step 8925, Loss 0.40444695949554443\n",
            "Step 8950, Loss 0.481269896030426\n",
            "Step 8975, Loss 0.37980175018310547\n",
            "Step 9000, Loss 0.30380621552467346\n",
            "Step 9025, Loss 0.3641267418861389\n",
            "Step 9050, Loss 0.4374273121356964\n",
            "Step 9075, Loss 0.4909347891807556\n",
            "Step 9100, Loss 0.4173806607723236\n",
            "Step 9125, Loss 0.3704933822154999\n",
            "Starting epoch 27/160, LR = [0.020610737385376336]\n",
            "Step 9150, Loss 0.4597111642360687\n",
            "Step 9175, Loss 0.5012759566307068\n",
            "Step 9200, Loss 0.5876274108886719\n",
            "Step 9225, Loss 0.45928293466567993\n",
            "Step 9250, Loss 0.5572083592414856\n",
            "Step 9275, Loss 0.491833359003067\n",
            "Step 9300, Loss 0.6073939204216003\n",
            "Step 9325, Loss 0.5301973819732666\n",
            "Step 9350, Loss 0.4484030604362488\n",
            "Step 9375, Loss 0.5106828212738037\n",
            "Step 9400, Loss 0.4759159982204437\n",
            "Step 9425, Loss 0.5070160627365112\n",
            "Step 9450, Loss 0.5269604921340942\n",
            "Step 9475, Loss 0.5040679574012756\n",
            "Starting epoch 28/160, LR = [0.027300475013022647]\n",
            "Step 9500, Loss 0.4703683853149414\n",
            "Step 9525, Loss 0.5050433278083801\n",
            "Step 9550, Loss 0.4251924455165863\n",
            "Step 9575, Loss 0.4515545070171356\n",
            "Step 9600, Loss 0.6984748244285583\n",
            "Step 9625, Loss 0.6808302402496338\n",
            "Step 9650, Loss 0.4210202693939209\n",
            "Step 9675, Loss 0.5756929516792297\n",
            "Step 9700, Loss 0.4314311146736145\n",
            "Step 9725, Loss 0.5616066455841064\n",
            "Step 9750, Loss 0.43370431661605835\n",
            "Step 9775, Loss 0.5437654852867126\n",
            "Step 9800, Loss 0.6822205781936646\n",
            "Step 9825, Loss 0.49163517355918884\n",
            "Starting epoch 29/160, LR = [0.03454915028125261]\n",
            "Step 9850, Loss 0.5142467021942139\n",
            "Step 9875, Loss 0.44382524490356445\n",
            "Step 9900, Loss 0.5377945899963379\n",
            "Step 9925, Loss 0.5918951630592346\n",
            "Step 9950, Loss 0.617860734462738\n",
            "Step 9975, Loss 0.51066654920578\n",
            "Step 10000, Loss 0.5583212375640869\n",
            "Step 10025, Loss 0.5487160682678223\n",
            "Step 10050, Loss 0.6910897493362427\n",
            "Step 10075, Loss 0.6337690949440002\n",
            "Step 10100, Loss 0.768904983997345\n",
            "Step 10125, Loss 0.6777752041816711\n",
            "Step 10150, Loss 0.6313350200653076\n",
            "Step 10175, Loss 0.689612090587616\n",
            "Starting epoch 30/160, LR = [0.042178276747988436]\n",
            "Step 10200, Loss 0.6171436905860901\n",
            "Step 10225, Loss 0.742644190788269\n",
            "Step 10250, Loss 0.6198368072509766\n",
            "Step 10275, Loss 0.6336988806724548\n",
            "Step 10300, Loss 0.7452160120010376\n",
            "Step 10325, Loss 0.6702611446380615\n",
            "Step 10350, Loss 0.6533804535865784\n",
            "Step 10375, Loss 0.6478908658027649\n",
            "Step 10400, Loss 0.7723584175109863\n",
            "Step 10425, Loss 0.7073589563369751\n",
            "Step 10450, Loss 0.753442645072937\n",
            "Step 10475, Loss 0.8959560394287109\n",
            "Step 10500, Loss 0.652001678943634\n",
            "Step 10525, Loss 0.5784250497817993\n",
            "Starting epoch 31/160, LR = [0.049999999999999975]\n",
            "Step 10550, Loss 0.6317123174667358\n",
            "Step 10575, Loss 0.6029680371284485\n",
            "Step 10600, Loss 0.6495752930641174\n",
            "Step 10625, Loss 0.6213133931159973\n",
            "Step 10650, Loss 0.8328460454940796\n",
            "Step 10675, Loss 0.8418983221054077\n",
            "Step 10700, Loss 0.7421690821647644\n",
            "Step 10725, Loss 0.8688110113143921\n",
            "Step 10750, Loss 0.7180708050727844\n",
            "Step 10775, Loss 0.8683679103851318\n",
            "Step 10800, Loss 0.7340123653411865\n",
            "Step 10825, Loss 0.6411542296409607\n",
            "Step 10850, Loss 0.7077497243881226\n",
            "Step 10875, Loss 0.6913113594055176\n",
            "Starting epoch 32/160, LR = [0.05782172325201152]\n",
            "Step 10900, Loss 0.5318339467048645\n",
            "Step 10925, Loss 0.7650219202041626\n",
            "Step 10950, Loss 0.7748305797576904\n",
            "Step 10975, Loss 1.0563225746154785\n",
            "Step 11000, Loss 0.7882457971572876\n",
            "Step 11025, Loss 0.7683863639831543\n",
            "Step 11050, Loss 0.7906690835952759\n",
            "Step 11075, Loss 1.0124627351760864\n",
            "Step 11100, Loss 0.904380202293396\n",
            "Step 11125, Loss 0.909776508808136\n",
            "Step 11150, Loss 0.9129023551940918\n",
            "Step 11175, Loss 0.8515071272850037\n",
            "Step 11200, Loss 1.0016250610351562\n",
            "Step 11225, Loss 1.014007329940796\n",
            "Starting epoch 33/160, LR = [0.06545084971874734]\n",
            "Step 11250, Loss 1.0053387880325317\n",
            "Step 11275, Loss 0.7903236150741577\n",
            "Step 11300, Loss 0.8401618599891663\n",
            "Step 11325, Loss 0.9701728820800781\n",
            "Step 11350, Loss 0.8029235005378723\n",
            "Step 11375, Loss 0.8383007049560547\n",
            "Step 11400, Loss 0.801317036151886\n",
            "Step 11425, Loss 0.9141608476638794\n",
            "Step 11450, Loss 0.8445186018943787\n",
            "Step 11475, Loss 0.691308856010437\n",
            "Step 11500, Loss 0.8042479157447815\n",
            "Step 11525, Loss 0.9450281858444214\n",
            "Step 11550, Loss 1.003731608390808\n",
            "Step 11575, Loss 0.8441686630249023\n",
            "Starting epoch 34/160, LR = [0.0726995249869773]\n",
            "Step 11600, Loss 0.7835939526557922\n",
            "Step 11625, Loss 0.8656432628631592\n",
            "Step 11650, Loss 0.7369415760040283\n",
            "Step 11675, Loss 0.9047243595123291\n",
            "Step 11700, Loss 0.8918181657791138\n",
            "Step 11725, Loss 0.9361141920089722\n",
            "Step 11750, Loss 0.9131101965904236\n",
            "Step 11775, Loss 0.9238466024398804\n",
            "Step 11800, Loss 0.8502448201179504\n",
            "Step 11825, Loss 0.9905926585197449\n",
            "Step 11850, Loss 1.154759168624878\n",
            "Step 11875, Loss 0.8752720952033997\n",
            "Step 11900, Loss 1.2203490734100342\n",
            "Step 11925, Loss 1.1808351278305054\n",
            "Starting epoch 35/160, LR = [0.0793892626146236]\n",
            "Step 11950, Loss 1.1430039405822754\n",
            "Step 11975, Loss 0.9954735636711121\n",
            "Step 12000, Loss 1.0698802471160889\n",
            "Step 12025, Loss 1.0645204782485962\n",
            "Step 12050, Loss 0.9032536149024963\n",
            "Step 12075, Loss 0.9943702816963196\n",
            "Step 12100, Loss 1.0180505514144897\n",
            "Step 12125, Loss 1.1348466873168945\n",
            "Step 12150, Loss 1.0482444763183594\n",
            "Step 12175, Loss 0.866406261920929\n",
            "Step 12200, Loss 1.1405363082885742\n",
            "Step 12225, Loss 0.9484878182411194\n",
            "Step 12250, Loss 1.103289246559143\n",
            "Step 12275, Loss 1.1043468713760376\n",
            "Starting epoch 36/160, LR = [0.08535533905932732]\n",
            "Step 12300, Loss 0.8531848788261414\n",
            "Step 12325, Loss 0.9869307279586792\n",
            "Step 12350, Loss 1.029437780380249\n",
            "Step 12375, Loss 0.9673756957054138\n",
            "Step 12400, Loss 1.0812664031982422\n",
            "Step 12425, Loss 1.0013248920440674\n",
            "Step 12450, Loss 0.9036957621574402\n",
            "Step 12475, Loss 1.1678227186203003\n",
            "Step 12500, Loss 1.245417833328247\n",
            "Step 12525, Loss 1.0295004844665527\n",
            "Step 12550, Loss 0.994196891784668\n",
            "Step 12575, Loss 0.9640077948570251\n",
            "Step 12600, Loss 1.007789969444275\n",
            "Step 12625, Loss 1.0629676580429077\n",
            "Starting epoch 37/160, LR = [0.09045084971874733]\n",
            "Step 12650, Loss 0.9968649744987488\n",
            "Step 12675, Loss 0.714432418346405\n",
            "Step 12700, Loss 1.21271812915802\n",
            "Step 12725, Loss 1.1329749822616577\n",
            "Step 12750, Loss 1.3075066804885864\n",
            "Step 12775, Loss 0.9252644777297974\n",
            "Step 12800, Loss 1.157760739326477\n",
            "Step 12825, Loss 1.233875036239624\n",
            "Step 12850, Loss 1.191312551498413\n",
            "Step 12875, Loss 1.1150753498077393\n",
            "Step 12900, Loss 0.8854982852935791\n",
            "Step 12925, Loss 1.2824727296829224\n",
            "Step 12950, Loss 1.1286756992340088\n",
            "Step 12975, Loss 1.0818229913711548\n",
            "Starting epoch 38/160, LR = [0.09455032620941833]\n",
            "Step 13000, Loss 0.7828986048698425\n",
            "Step 13025, Loss 0.9597113132476807\n",
            "Step 13050, Loss 0.9433582425117493\n",
            "Step 13075, Loss 1.1590735912322998\n",
            "Step 13100, Loss 1.0803146362304688\n",
            "Step 13125, Loss 1.1178525686264038\n",
            "Step 13150, Loss 1.1996159553527832\n",
            "Step 13175, Loss 0.9482612609863281\n",
            "Step 13200, Loss 0.94712233543396\n",
            "Step 13225, Loss 1.4660093784332275\n",
            "Step 13250, Loss 1.151949167251587\n",
            "Step 13275, Loss 1.0453152656555176\n",
            "Step 13300, Loss 0.9279062747955322\n",
            "Step 13325, Loss 1.1836469173431396\n",
            "Starting epoch 39/160, LR = [0.09755282581475762]\n",
            "Step 13350, Loss 1.0503191947937012\n",
            "Step 13375, Loss 0.8941478729248047\n",
            "Step 13400, Loss 0.9439557194709778\n",
            "Step 13425, Loss 1.1608481407165527\n",
            "Step 13450, Loss 0.999774158000946\n",
            "Step 13475, Loss 1.093152403831482\n",
            "Step 13500, Loss 0.9936383962631226\n",
            "Step 13525, Loss 1.3035526275634766\n",
            "Step 13550, Loss 1.075384259223938\n",
            "Step 13575, Loss 1.2097351551055908\n",
            "Step 13600, Loss 0.9235692620277405\n",
            "Step 13625, Loss 0.958136796951294\n",
            "Step 13650, Loss 1.2378597259521484\n",
            "Step 13675, Loss 1.197394609451294\n",
            "Starting epoch 40/160, LR = [0.09938441702975682]\n",
            "Step 13700, Loss 0.9048943519592285\n",
            "Step 13725, Loss 0.9394376873970032\n",
            "Step 13750, Loss 0.8228803873062134\n",
            "Step 13775, Loss 1.162583827972412\n",
            "Step 13800, Loss 0.9778010845184326\n",
            "Step 13825, Loss 1.152230978012085\n",
            "Step 13850, Loss 1.2032514810562134\n",
            "Step 13875, Loss 0.9962130784988403\n",
            "Step 13900, Loss 1.16090726852417\n",
            "Step 13925, Loss 1.0249494314193726\n",
            "Step 13950, Loss 1.166857361793518\n",
            "Step 13975, Loss 1.0817304849624634\n",
            "Step 14000, Loss 0.8809915781021118\n",
            "Step 14025, Loss 1.1060601472854614\n",
            "Starting epoch 41/160, LR = [0.09999999999999995]\n",
            "Step 14050, Loss 0.9903439879417419\n",
            "Step 14075, Loss 1.012376070022583\n",
            "Step 14100, Loss 1.0115242004394531\n",
            "Step 14125, Loss 1.187570571899414\n",
            "Step 14150, Loss 0.9642544984817505\n",
            "Step 14175, Loss 0.8394114375114441\n",
            "Step 14200, Loss 1.3648481369018555\n",
            "Step 14225, Loss 1.0866032838821411\n",
            "Step 14250, Loss 1.2525858879089355\n",
            "Step 14275, Loss 1.1310052871704102\n",
            "Step 14300, Loss 1.1652555465698242\n",
            "Step 14325, Loss 1.0149356126785278\n",
            "Step 14350, Loss 1.1732969284057617\n",
            "Step 14375, Loss 1.042767882347107\n",
            "Starting epoch 42/160, LR = [0.09938441702975684]\n",
            "Step 14400, Loss 0.824573814868927\n",
            "Step 14425, Loss 0.870697021484375\n",
            "Step 14450, Loss 0.9816187620162964\n",
            "Step 14475, Loss 0.9477005004882812\n",
            "Step 14500, Loss 0.9257659912109375\n",
            "Step 14525, Loss 1.239341378211975\n",
            "Step 14550, Loss 0.8330686688423157\n",
            "Step 14575, Loss 0.921575665473938\n",
            "Step 14600, Loss 1.075525164604187\n",
            "Step 14625, Loss 1.1318745613098145\n",
            "Step 14650, Loss 1.24265456199646\n",
            "Step 14675, Loss 0.9896891117095947\n",
            "Step 14700, Loss 1.0912410020828247\n",
            "Step 14725, Loss 1.0820130109786987\n",
            "Starting epoch 43/160, LR = [0.09755282581475763]\n",
            "Step 14750, Loss 0.9808096289634705\n",
            "Step 14775, Loss 0.805929958820343\n",
            "Step 14800, Loss 0.8393833637237549\n",
            "Step 14825, Loss 0.997226893901825\n",
            "Step 14850, Loss 0.8589885234832764\n",
            "Step 14875, Loss 0.9513136744499207\n",
            "Step 14900, Loss 0.9066054821014404\n",
            "Step 14925, Loss 1.0690137147903442\n",
            "Step 14950, Loss 1.0602394342422485\n",
            "Step 14975, Loss 0.9378901124000549\n",
            "Step 15000, Loss 1.1811788082122803\n",
            "Step 15025, Loss 1.1577188968658447\n",
            "Step 15050, Loss 1.283937931060791\n",
            "Step 15075, Loss 0.8738088607788086\n",
            "Starting epoch 44/160, LR = [0.09455032620941835]\n",
            "Step 15100, Loss 0.7952701449394226\n",
            "Step 15125, Loss 1.0823872089385986\n",
            "Step 15150, Loss 0.7658101320266724\n",
            "Step 15175, Loss 0.9400247931480408\n",
            "Step 15200, Loss 0.9339760541915894\n",
            "Step 15225, Loss 1.0621154308319092\n",
            "Step 15250, Loss 0.9864649772644043\n",
            "Step 15275, Loss 0.8157608509063721\n",
            "Step 15300, Loss 1.0082998275756836\n",
            "Step 15325, Loss 1.1421242952346802\n",
            "Step 15350, Loss 1.1579334735870361\n",
            "Step 15375, Loss 0.8271970748901367\n",
            "Step 15400, Loss 0.8954538106918335\n",
            "Step 15425, Loss 1.248255729675293\n",
            "Starting epoch 45/160, LR = [0.09045084971874735]\n",
            "Step 15450, Loss 1.0009925365447998\n",
            "Step 15475, Loss 0.8714385032653809\n",
            "Step 15500, Loss 0.8091599941253662\n",
            "Step 15525, Loss 0.7144607305526733\n",
            "Step 15550, Loss 1.1040085554122925\n",
            "Step 15575, Loss 0.9553571343421936\n",
            "Step 15600, Loss 1.0946763753890991\n",
            "Step 15625, Loss 1.0315802097320557\n",
            "Step 15650, Loss 0.9897368550300598\n",
            "Step 15675, Loss 1.001723289489746\n",
            "Step 15700, Loss 1.0730390548706055\n",
            "Step 15725, Loss 1.0810054540634155\n",
            "Step 15750, Loss 0.8501484990119934\n",
            "Step 15775, Loss 0.998094916343689\n",
            "Starting epoch 46/160, LR = [0.08535533905932734]\n",
            "Step 15800, Loss 0.9236512184143066\n",
            "Step 15825, Loss 1.041187047958374\n",
            "Step 15850, Loss 0.8841607570648193\n",
            "Step 15875, Loss 0.7767429351806641\n",
            "Step 15900, Loss 0.8592889904975891\n",
            "Step 15925, Loss 1.0613666772842407\n",
            "Step 15950, Loss 0.9770447611808777\n",
            "Step 15975, Loss 0.8095749616622925\n",
            "Step 16000, Loss 1.070369005203247\n",
            "Step 16025, Loss 0.9273727536201477\n",
            "Step 16050, Loss 0.7323229312896729\n",
            "Step 16075, Loss 0.917170524597168\n",
            "Step 16100, Loss 0.8892651796340942\n",
            "Step 16125, Loss 1.114004135131836\n",
            "Starting epoch 47/160, LR = [0.07938926261462363]\n",
            "Step 16150, Loss 0.8791742324829102\n",
            "Step 16175, Loss 0.7307380437850952\n",
            "Step 16200, Loss 0.9379804730415344\n",
            "Step 16225, Loss 0.7671272158622742\n",
            "Step 16250, Loss 0.9545828104019165\n",
            "Step 16275, Loss 0.9102170467376709\n",
            "Step 16300, Loss 0.8575259447097778\n",
            "Step 16325, Loss 0.850387454032898\n",
            "Step 16350, Loss 0.8796714544296265\n",
            "Step 16375, Loss 0.9545982480049133\n",
            "Step 16400, Loss 0.745684802532196\n",
            "Step 16425, Loss 0.9105053544044495\n",
            "Step 16450, Loss 0.7057090997695923\n",
            "Step 16475, Loss 0.8817967176437378\n",
            "Starting epoch 48/160, LR = [0.07269952498697728]\n",
            "Step 16500, Loss 0.7484697103500366\n",
            "Step 16525, Loss 0.926748514175415\n",
            "Step 16550, Loss 0.9467597007751465\n",
            "Step 16575, Loss 0.7988673448562622\n",
            "Step 16600, Loss 1.0228934288024902\n",
            "Step 16625, Loss 1.0248525142669678\n",
            "Step 16650, Loss 0.9313156604766846\n",
            "Step 16675, Loss 0.9300075173377991\n",
            "Step 16700, Loss 0.7341261506080627\n",
            "Step 16725, Loss 1.137323260307312\n",
            "Step 16750, Loss 0.8373955488204956\n",
            "Step 16775, Loss 0.9767991304397583\n",
            "Step 16800, Loss 0.7275665998458862\n",
            "Step 16825, Loss 0.8998567461967468\n",
            "Starting epoch 49/160, LR = [0.06545084971874734]\n",
            "Step 16850, Loss 0.793535053730011\n",
            "Step 16875, Loss 0.783715009689331\n",
            "Step 16900, Loss 0.7352334260940552\n",
            "Step 16925, Loss 0.6787543892860413\n",
            "Step 16950, Loss 0.7929535508155823\n",
            "Step 16975, Loss 0.8808276653289795\n",
            "Step 17000, Loss 0.9577142000198364\n",
            "Step 17025, Loss 0.7280762791633606\n",
            "Step 17050, Loss 0.8578933477401733\n",
            "Step 17075, Loss 0.8087668418884277\n",
            "Step 17100, Loss 0.8116440773010254\n",
            "Step 17125, Loss 0.7607308626174927\n",
            "Step 17150, Loss 0.7861204743385315\n",
            "Step 17175, Loss 0.6967726349830627\n",
            "Starting epoch 50/160, LR = [0.05782172325201157]\n",
            "Step 17200, Loss 0.7870303988456726\n",
            "Step 17225, Loss 0.7986964583396912\n",
            "Step 17250, Loss 0.710214376449585\n",
            "Step 17275, Loss 0.881374716758728\n",
            "Step 17300, Loss 0.7166227698326111\n",
            "Step 17325, Loss 0.8245072960853577\n",
            "Step 17350, Loss 0.7552481293678284\n",
            "Step 17375, Loss 0.8027901649475098\n",
            "Step 17400, Loss 0.8578237891197205\n",
            "Step 17425, Loss 0.9574719667434692\n",
            "Step 17450, Loss 0.5198307037353516\n",
            "Step 17475, Loss 0.881357729434967\n",
            "Step 17500, Loss 0.744620144367218\n",
            "Step 17525, Loss 0.838824987411499\n",
            "Starting epoch 51/160, LR = [0.04999999999999998]\n",
            "Step 17550, Loss 0.7947885394096375\n",
            "Step 17575, Loss 0.4602571129798889\n",
            "Step 17600, Loss 0.7423994541168213\n",
            "Step 17625, Loss 0.7715671062469482\n",
            "Step 17650, Loss 0.5385992527008057\n",
            "Step 17675, Loss 0.6585640907287598\n",
            "Step 17700, Loss 0.7715253233909607\n",
            "Step 17725, Loss 0.6326445937156677\n",
            "Step 17750, Loss 0.8737779259681702\n",
            "Step 17775, Loss 0.748223066329956\n",
            "Step 17800, Loss 0.6794460415840149\n",
            "Step 17825, Loss 0.7123958468437195\n",
            "Step 17850, Loss 0.6959295868873596\n",
            "Step 17875, Loss 0.6468594074249268\n",
            "Step 17900, Loss 0.6286101937294006\n",
            "Starting epoch 52/160, LR = [0.04217827674798849]\n",
            "Step 17925, Loss 0.6289346218109131\n",
            "Step 17950, Loss 0.5175818204879761\n",
            "Step 17975, Loss 0.48009824752807617\n",
            "Step 18000, Loss 0.5018569231033325\n",
            "Step 18025, Loss 0.5440812110900879\n",
            "Step 18050, Loss 0.7708922028541565\n",
            "Step 18075, Loss 0.6117982864379883\n",
            "Step 18100, Loss 0.8088057041168213\n",
            "Step 18125, Loss 0.5539497137069702\n",
            "Step 18150, Loss 0.6139064431190491\n",
            "Step 18175, Loss 0.6682462096214294\n",
            "Step 18200, Loss 0.706864595413208\n",
            "Step 18225, Loss 0.6566898226737976\n",
            "Step 18250, Loss 0.6939733028411865\n",
            "Starting epoch 53/160, LR = [0.03454915028125262]\n",
            "Step 18275, Loss 0.5882788896560669\n",
            "Step 18300, Loss 0.5759521126747131\n",
            "Step 18325, Loss 0.5029463171958923\n",
            "Step 18350, Loss 0.6798651814460754\n",
            "Step 18375, Loss 0.6041950583457947\n",
            "Step 18400, Loss 0.6255634427070618\n",
            "Step 18425, Loss 0.4918738305568695\n",
            "Step 18450, Loss 0.5786828398704529\n",
            "Step 18475, Loss 0.5502288937568665\n",
            "Step 18500, Loss 0.5892284512519836\n",
            "Step 18525, Loss 0.707115650177002\n",
            "Step 18550, Loss 0.6123701930046082\n",
            "Step 18575, Loss 0.6772387623786926\n",
            "Step 18600, Loss 0.781997799873352\n",
            "Starting epoch 54/160, LR = [0.027300475013022612]\n",
            "Step 18625, Loss 0.5026227831840515\n",
            "Step 18650, Loss 0.5397259593009949\n",
            "Step 18675, Loss 0.4418083727359772\n",
            "Step 18700, Loss 0.5574249625205994\n",
            "Step 18725, Loss 0.5889672040939331\n",
            "Step 18750, Loss 0.5834575891494751\n",
            "Step 18775, Loss 0.5341848134994507\n",
            "Step 18800, Loss 0.47260919213294983\n",
            "Step 18825, Loss 0.5284481644630432\n",
            "Step 18850, Loss 0.5520868897438049\n",
            "Step 18875, Loss 0.6581382155418396\n",
            "Step 18900, Loss 0.5261073112487793\n",
            "Step 18925, Loss 0.6038553714752197\n",
            "Step 18950, Loss 0.5803790092468262\n",
            "Starting epoch 55/160, LR = [0.02061073738537634]\n",
            "Step 18975, Loss 0.4720200002193451\n",
            "Step 19000, Loss 0.6353949904441833\n",
            "Step 19025, Loss 0.4423314034938812\n",
            "Step 19050, Loss 0.5453906059265137\n",
            "Step 19075, Loss 0.5825484395027161\n",
            "Step 19100, Loss 0.5186848044395447\n",
            "Step 19125, Loss 0.5618481636047363\n",
            "Step 19150, Loss 0.48468178510665894\n",
            "Step 19175, Loss 0.5408517718315125\n",
            "Step 19200, Loss 0.5556727051734924\n",
            "Step 19225, Loss 0.4617435932159424\n",
            "Step 19250, Loss 0.47940027713775635\n",
            "Step 19275, Loss 0.7715168595314026\n",
            "Step 19300, Loss 0.5144578814506531\n",
            "Starting epoch 56/160, LR = [0.014644660940672594]\n",
            "Step 19325, Loss 0.4094482362270355\n",
            "Step 19350, Loss 0.5064762234687805\n",
            "Step 19375, Loss 0.5032963752746582\n",
            "Step 19400, Loss 0.47968173027038574\n",
            "Step 19425, Loss 0.41298389434814453\n",
            "Step 19450, Loss 0.39449918270111084\n",
            "Step 19475, Loss 0.33213022351264954\n",
            "Step 19500, Loss 0.49683016538619995\n",
            "Step 19525, Loss 0.41331958770751953\n",
            "Step 19550, Loss 0.4605990946292877\n",
            "Step 19575, Loss 0.40217289328575134\n",
            "Step 19600, Loss 0.5855730175971985\n",
            "Step 19625, Loss 0.27266913652420044\n",
            "Step 19650, Loss 0.44572028517723083\n",
            "Starting epoch 57/160, LR = [0.009549150281252632]\n",
            "Step 19675, Loss 0.3423598110675812\n",
            "Step 19700, Loss 0.6435002088546753\n",
            "Step 19725, Loss 0.2956986129283905\n",
            "Step 19750, Loss 0.43241116404533386\n",
            "Step 19775, Loss 0.5095900893211365\n",
            "Step 19800, Loss 0.33128488063812256\n",
            "Step 19825, Loss 0.3637068271636963\n",
            "Step 19850, Loss 0.4969155490398407\n",
            "Step 19875, Loss 0.5183854103088379\n",
            "Step 19900, Loss 0.4347602128982544\n",
            "Step 19925, Loss 0.4593268036842346\n",
            "Step 19950, Loss 0.45767742395401\n",
            "Step 19975, Loss 0.3864763379096985\n",
            "Step 20000, Loss 0.4170750677585602\n",
            "Starting epoch 58/160, LR = [0.005449673790581629]\n",
            "Step 20025, Loss 0.3359832763671875\n",
            "Step 20050, Loss 0.39226558804512024\n",
            "Step 20075, Loss 0.3596794009208679\n",
            "Step 20100, Loss 0.287952184677124\n",
            "Step 20125, Loss 0.45031705498695374\n",
            "Step 20150, Loss 0.3544175922870636\n",
            "Step 20175, Loss 0.4033600986003876\n",
            "Step 20200, Loss 0.35871654748916626\n",
            "Step 20225, Loss 0.3653259873390198\n",
            "Step 20250, Loss 0.36585164070129395\n",
            "Step 20275, Loss 0.45172566175460815\n",
            "Step 20300, Loss 0.3424244821071625\n",
            "Step 20325, Loss 0.40351712703704834\n",
            "Step 20350, Loss 0.38316866755485535\n",
            "Starting epoch 59/160, LR = [0.0024471741852423274]\n",
            "Step 20375, Loss 0.32407933473587036\n",
            "Step 20400, Loss 0.343596875667572\n",
            "Step 20425, Loss 0.3464260399341583\n",
            "Step 20450, Loss 0.4557468295097351\n",
            "Step 20475, Loss 0.32935193181037903\n",
            "Step 20500, Loss 0.35473766922950745\n",
            "Step 20525, Loss 0.3725232481956482\n",
            "Step 20550, Loss 0.34851396083831787\n",
            "Step 20575, Loss 0.45339083671569824\n",
            "Step 20600, Loss 0.2897077798843384\n",
            "Step 20625, Loss 0.3489079475402832\n",
            "Step 20650, Loss 0.35858047008514404\n",
            "Step 20675, Loss 0.3616942763328552\n",
            "Step 20700, Loss 0.3212434947490692\n",
            "Starting epoch 60/160, LR = [0.0006155829702431222]\n",
            "Step 20725, Loss 0.34210747480392456\n",
            "Step 20750, Loss 0.39785781502723694\n",
            "Step 20775, Loss 0.25762850046157837\n",
            "Step 20800, Loss 0.31566721200942993\n",
            "Step 20825, Loss 0.36741721630096436\n",
            "Step 20850, Loss 0.45412778854370117\n",
            "Step 20875, Loss 0.3328910171985626\n",
            "Step 20900, Loss 0.42838045954704285\n",
            "Step 20925, Loss 0.40477636456489563\n",
            "Step 20950, Loss 0.35272055864334106\n",
            "Step 20975, Loss 0.44190144538879395\n",
            "Step 21000, Loss 0.3144104480743408\n",
            "Step 21025, Loss 0.40071016550064087\n",
            "Step 21050, Loss 0.3247268795967102\n",
            "Starting epoch 61/160, LR = [0.0]\n",
            "Step 21075, Loss 0.29829537868499756\n",
            "Step 21100, Loss 0.39351046085357666\n",
            "Step 21125, Loss 0.312898188829422\n",
            "Step 21150, Loss 0.2709091007709503\n",
            "Step 21175, Loss 0.35592028498649597\n",
            "Step 21200, Loss 0.3846871554851532\n",
            "Step 21225, Loss 0.33340880274772644\n",
            "Step 21250, Loss 0.3330904543399811\n",
            "Step 21275, Loss 0.4097267687320709\n",
            "Step 21300, Loss 0.4748709797859192\n",
            "Step 21325, Loss 0.3857120871543884\n",
            "Step 21350, Loss 0.3342916965484619\n",
            "Step 21375, Loss 0.3696220815181732\n",
            "Step 21400, Loss 0.3579506576061249\n",
            "Starting epoch 62/160, LR = [0.0006155829702431115]\n",
            "Step 21425, Loss 0.45822978019714355\n",
            "Step 21450, Loss 0.461749792098999\n",
            "Step 21475, Loss 0.4469990134239197\n",
            "Step 21500, Loss 0.41343510150909424\n",
            "Step 21525, Loss 0.2938195466995239\n",
            "Step 21550, Loss 0.3172058165073395\n",
            "Step 21575, Loss 0.29219043254852295\n",
            "Step 21600, Loss 0.33323734998703003\n",
            "Step 21625, Loss 0.4061697721481323\n",
            "Step 21650, Loss 0.3588983416557312\n",
            "Step 21675, Loss 0.3786841034889221\n",
            "Step 21700, Loss 0.399968683719635\n",
            "Step 21725, Loss 0.3957192003726959\n",
            "Step 21750, Loss 0.3499773144721985\n",
            "Starting epoch 63/160, LR = [0.0024471741852422958]\n",
            "Step 21775, Loss 0.3149099349975586\n",
            "Step 21800, Loss 0.3676353394985199\n",
            "Step 21825, Loss 0.38450050354003906\n",
            "Step 21850, Loss 0.33503925800323486\n",
            "Step 21875, Loss 0.4509957432746887\n",
            "Step 21900, Loss 0.27720457315444946\n",
            "Step 21925, Loss 0.41593030095100403\n",
            "Step 21950, Loss 0.32884854078292847\n",
            "Step 21975, Loss 0.41778630018234253\n",
            "Step 22000, Loss 0.3800926208496094\n",
            "Step 22025, Loss 0.4307623505592346\n",
            "Step 22050, Loss 0.3655411899089813\n",
            "Step 22075, Loss 0.3859037160873413\n",
            "Step 22100, Loss 0.4918729364871979\n",
            "Starting epoch 64/160, LR = [0.005449673790581567]\n",
            "Step 22125, Loss 0.24253006279468536\n",
            "Step 22150, Loss 0.31364014744758606\n",
            "Step 22175, Loss 0.23199915885925293\n",
            "Step 22200, Loss 0.43379712104797363\n",
            "Step 22225, Loss 0.3926165699958801\n",
            "Step 22250, Loss 0.3947690725326538\n",
            "Step 22275, Loss 0.4678876996040344\n",
            "Step 22300, Loss 0.3263886272907257\n",
            "Step 22325, Loss 0.2705165147781372\n",
            "Step 22350, Loss 0.3504922688007355\n",
            "Step 22375, Loss 0.4416332542896271\n",
            "Step 22400, Loss 0.5622917413711548\n",
            "Step 22425, Loss 0.36876142024993896\n",
            "Step 22450, Loss 0.3924230635166168\n",
            "Starting epoch 65/160, LR = [0.00954915028125253]\n",
            "Step 22475, Loss 0.3338536024093628\n",
            "Step 22500, Loss 0.40066593885421753\n",
            "Step 22525, Loss 0.410869300365448\n",
            "Step 22550, Loss 0.26337650418281555\n",
            "Step 22575, Loss 0.26996368169784546\n",
            "Step 22600, Loss 0.3327406048774719\n",
            "Step 22625, Loss 0.4181300401687622\n",
            "Step 22650, Loss 0.5002073049545288\n",
            "Step 22675, Loss 0.32518452405929565\n",
            "Step 22700, Loss 0.3983111083507538\n",
            "Step 22725, Loss 0.4224504828453064\n",
            "Step 22750, Loss 0.5297552347183228\n",
            "Step 22775, Loss 0.3283611238002777\n",
            "Step 22800, Loss 0.22798682749271393\n",
            "Starting epoch 66/160, LR = [0.014644660940672443]\n",
            "Step 22825, Loss 0.3615536689758301\n",
            "Step 22850, Loss 0.2847474217414856\n",
            "Step 22875, Loss 0.44415560364723206\n",
            "Step 22900, Loss 0.3825712203979492\n",
            "Step 22925, Loss 0.3694130778312683\n",
            "Step 22950, Loss 0.4080687165260315\n",
            "Step 22975, Loss 0.4190509021282196\n",
            "Step 23000, Loss 0.4291251599788666\n",
            "Step 23025, Loss 0.4254333972930908\n",
            "Step 23050, Loss 0.4810096323490143\n",
            "Step 23075, Loss 0.5278109312057495\n",
            "Step 23100, Loss 0.396104633808136\n",
            "Step 23125, Loss 0.5562248229980469\n",
            "Step 23150, Loss 0.35664311051368713\n",
            "Starting epoch 67/160, LR = [0.020610737385376138]\n",
            "Step 23175, Loss 0.3373560607433319\n",
            "Step 23200, Loss 0.4328414797782898\n",
            "Step 23225, Loss 0.37637045979499817\n",
            "Step 23250, Loss 0.5343060493469238\n",
            "Step 23275, Loss 0.48582687973976135\n",
            "Step 23300, Loss 0.5604774951934814\n",
            "Step 23325, Loss 0.48131129145622253\n",
            "Step 23350, Loss 0.5044399499893188\n",
            "Step 23375, Loss 0.41608259081840515\n",
            "Step 23400, Loss 0.4734943211078644\n",
            "Step 23425, Loss 0.5340986251831055\n",
            "Step 23450, Loss 0.33958715200424194\n",
            "Step 23475, Loss 0.48998695611953735\n",
            "Step 23500, Loss 0.49662312865257263\n",
            "Starting epoch 68/160, LR = [0.02730047501302235]\n",
            "Step 23525, Loss 0.4021746516227722\n",
            "Step 23550, Loss 0.557945728302002\n",
            "Step 23575, Loss 0.4582626223564148\n",
            "Step 23600, Loss 0.500557541847229\n",
            "Step 23625, Loss 0.6050766110420227\n",
            "Step 23650, Loss 0.65276038646698\n",
            "Step 23675, Loss 0.43060940504074097\n",
            "Step 23700, Loss 0.6243311166763306\n",
            "Step 23725, Loss 0.6037970781326294\n",
            "Step 23750, Loss 0.39585793018341064\n",
            "Step 23775, Loss 0.6131856441497803\n",
            "Step 23800, Loss 0.43112194538116455\n",
            "Step 23825, Loss 0.3445979058742523\n",
            "Step 23850, Loss 0.49010905623435974\n",
            "Starting epoch 69/160, LR = [0.034549150281252286]\n",
            "Step 23875, Loss 0.3637683093547821\n",
            "Step 23900, Loss 0.6429851055145264\n",
            "Step 23925, Loss 0.46153515577316284\n",
            "Step 23950, Loss 0.7165015339851379\n",
            "Step 23975, Loss 0.4951905906200409\n",
            "Step 24000, Loss 0.6679670810699463\n",
            "Step 24025, Loss 0.7809732556343079\n",
            "Step 24050, Loss 0.6724628210067749\n",
            "Step 24075, Loss 0.40581560134887695\n",
            "Step 24100, Loss 0.6523765325546265\n",
            "Step 24125, Loss 0.5270500779151917\n",
            "Step 24150, Loss 0.4899999797344208\n",
            "Step 24175, Loss 0.5655802488327026\n",
            "Step 24200, Loss 0.6397560238838196\n",
            "Starting epoch 70/160, LR = [0.042178276747988096]\n",
            "Step 24225, Loss 0.6230311989784241\n",
            "Step 24250, Loss 0.5799693465232849\n",
            "Step 24275, Loss 0.3855345547199249\n",
            "Step 24300, Loss 0.7741984724998474\n",
            "Step 24325, Loss 0.6855767965316772\n",
            "Step 24350, Loss 0.7587330341339111\n",
            "Step 24375, Loss 0.6366308927536011\n",
            "Step 24400, Loss 0.6037025451660156\n",
            "Step 24425, Loss 0.5681551694869995\n",
            "Step 24450, Loss 0.6110467314720154\n",
            "Step 24475, Loss 0.7323756814002991\n",
            "Step 24500, Loss 0.7475343942642212\n",
            "Step 24525, Loss 0.65218585729599\n",
            "Step 24550, Loss 0.6258583664894104\n",
            "Starting epoch 71/160, LR = [0.049999999999999524]\n",
            "Step 24575, Loss 0.6277093291282654\n",
            "Step 24600, Loss 0.6698474287986755\n",
            "Step 24625, Loss 0.6709972620010376\n",
            "Step 24650, Loss 0.7354974150657654\n",
            "Step 24675, Loss 0.7744055986404419\n",
            "Step 24700, Loss 0.6227092146873474\n",
            "Step 24725, Loss 0.768121063709259\n",
            "Step 24750, Loss 0.6588339805603027\n",
            "Step 24775, Loss 0.9197485446929932\n",
            "Step 24800, Loss 0.9032082557678223\n",
            "Step 24825, Loss 0.8432654738426208\n",
            "Step 24850, Loss 0.7544946074485779\n",
            "Step 24875, Loss 0.9689198732376099\n",
            "Step 24900, Loss 0.6743847727775574\n",
            "Starting epoch 72/160, LR = [0.05782172325201104]\n",
            "Step 24925, Loss 0.7827401161193848\n",
            "Step 24950, Loss 0.7567254304885864\n",
            "Step 24975, Loss 0.6790230870246887\n",
            "Step 25000, Loss 0.9193178415298462\n",
            "Step 25025, Loss 0.8185268640518188\n",
            "Step 25050, Loss 1.0090925693511963\n",
            "Step 25075, Loss 0.78704434633255\n",
            "Step 25100, Loss 0.9328036308288574\n",
            "Step 25125, Loss 0.8728313446044922\n",
            "Step 25150, Loss 0.6122824549674988\n",
            "Step 25175, Loss 0.8759847283363342\n",
            "Step 25200, Loss 0.7149418592453003\n",
            "Step 25225, Loss 0.9904678463935852\n",
            "Step 25250, Loss 0.9183268547058105\n",
            "Starting epoch 73/160, LR = [0.06545084971874676]\n",
            "Step 25275, Loss 0.7125781178474426\n",
            "Step 25300, Loss 0.7233888506889343\n",
            "Step 25325, Loss 0.8248429298400879\n",
            "Step 25350, Loss 0.7158315181732178\n",
            "Step 25375, Loss 0.912034273147583\n",
            "Step 25400, Loss 0.8821620941162109\n",
            "Step 25425, Loss 0.8496109247207642\n",
            "Step 25450, Loss 1.2271716594696045\n",
            "Step 25475, Loss 0.8012456893920898\n",
            "Step 25500, Loss 0.8935652375221252\n",
            "Step 25525, Loss 1.030613660812378\n",
            "Step 25550, Loss 0.8807395100593567\n",
            "Step 25575, Loss 0.9776939153671265\n",
            "Step 25600, Loss 0.9035447239875793\n",
            "Starting epoch 74/160, LR = [0.07269952498697663]\n",
            "Step 25625, Loss 0.8221462368965149\n",
            "Step 25650, Loss 1.0178637504577637\n",
            "Step 25675, Loss 0.9091324806213379\n",
            "Step 25700, Loss 1.0672117471694946\n",
            "Step 25725, Loss 1.1088941097259521\n",
            "Step 25750, Loss 0.8844518065452576\n",
            "Step 25775, Loss 0.9378176927566528\n",
            "Step 25800, Loss 0.8970681428909302\n",
            "Step 25825, Loss 1.0242069959640503\n",
            "Step 25850, Loss 1.1867269277572632\n",
            "Step 25875, Loss 0.7618169784545898\n",
            "Step 25900, Loss 1.1733696460723877\n",
            "Step 25925, Loss 0.8441596031188965\n",
            "Step 25950, Loss 1.0154777765274048\n",
            "Starting epoch 75/160, LR = [0.07938926261462294]\n",
            "Step 25975, Loss 0.8446248173713684\n",
            "Step 26000, Loss 0.6201669573783875\n",
            "Step 26025, Loss 0.8537498712539673\n",
            "Step 26050, Loss 1.080752968788147\n",
            "Step 26075, Loss 1.0304086208343506\n",
            "Step 26100, Loss 0.9573572874069214\n",
            "Step 26125, Loss 0.7964082956314087\n",
            "Step 26150, Loss 0.8742929100990295\n",
            "Step 26175, Loss 1.287978172302246\n",
            "Step 26200, Loss 1.182614803314209\n",
            "Step 26225, Loss 1.0606420040130615\n",
            "Step 26250, Loss 0.7328920960426331\n",
            "Step 26275, Loss 1.0084651708602905\n",
            "Step 26300, Loss 0.9527413249015808\n",
            "Starting epoch 76/160, LR = [0.08535533905932657]\n",
            "Step 26325, Loss 0.8882397413253784\n",
            "Step 26350, Loss 0.7576574683189392\n",
            "Step 26375, Loss 0.9737602472305298\n",
            "Step 26400, Loss 0.967068612575531\n",
            "Step 26425, Loss 0.971114456653595\n",
            "Step 26450, Loss 1.1487153768539429\n",
            "Step 26475, Loss 1.0156519412994385\n",
            "Step 26500, Loss 1.0125535726547241\n",
            "Step 26525, Loss 1.0235650539398193\n",
            "Step 26550, Loss 0.966347873210907\n",
            "Step 26575, Loss 1.1150261163711548\n",
            "Step 26600, Loss 1.1785954236984253\n",
            "Step 26625, Loss 0.9210715293884277\n",
            "Step 26650, Loss 0.9464536905288696\n",
            "Step 26675, Loss 1.026573896408081\n",
            "Starting epoch 77/160, LR = [0.09045084971874655]\n",
            "Step 26700, Loss 0.8770545125007629\n",
            "Step 26725, Loss 1.09254789352417\n",
            "Step 26750, Loss 0.8844805359840393\n",
            "Step 26775, Loss 1.4350433349609375\n",
            "Step 26800, Loss 0.915148138999939\n",
            "Step 26825, Loss 0.9921795725822449\n",
            "Step 26850, Loss 0.8370319604873657\n",
            "Step 26875, Loss 0.9359425902366638\n",
            "Step 26900, Loss 1.229238748550415\n",
            "Step 26925, Loss 1.073836088180542\n",
            "Step 26950, Loss 0.8602772951126099\n",
            "Step 26975, Loss 0.853426992893219\n",
            "Step 27000, Loss 1.0089452266693115\n",
            "Step 27025, Loss 1.1254057884216309\n",
            "Starting epoch 78/160, LR = [0.09455032620941757]\n",
            "Step 27050, Loss 0.8042968511581421\n",
            "Step 27075, Loss 1.3783239126205444\n",
            "Step 27100, Loss 0.8680046796798706\n",
            "Step 27125, Loss 1.006332278251648\n",
            "Step 27150, Loss 1.178810715675354\n",
            "Step 27175, Loss 0.9445846080780029\n",
            "Step 27200, Loss 1.0739717483520508\n",
            "Step 27225, Loss 1.2173396348953247\n",
            "Step 27250, Loss 0.9628306031227112\n",
            "Step 27275, Loss 0.8746240139007568\n",
            "Step 27300, Loss 1.0055001974105835\n",
            "Step 27325, Loss 1.0937535762786865\n",
            "Step 27350, Loss 0.9062694907188416\n",
            "Step 27375, Loss 0.9897174835205078\n",
            "Starting epoch 79/160, LR = [0.09755282581475681]\n",
            "Step 27400, Loss 1.1554715633392334\n",
            "Step 27425, Loss 1.0641833543777466\n",
            "Step 27450, Loss 0.8431640863418579\n",
            "Step 27475, Loss 0.9824457168579102\n",
            "Step 27500, Loss 1.4277724027633667\n",
            "Step 27525, Loss 1.022672176361084\n",
            "Step 27550, Loss 1.0491029024124146\n",
            "Step 27575, Loss 1.3722344636917114\n",
            "Step 27600, Loss 0.8939889073371887\n",
            "Step 27625, Loss 0.941152036190033\n",
            "Step 27650, Loss 1.404292106628418\n",
            "Step 27675, Loss 1.1039687395095825\n",
            "Step 27700, Loss 1.1691045761108398\n",
            "Step 27725, Loss 1.189964771270752\n",
            "Starting epoch 80/160, LR = [0.099384417029756]\n",
            "Step 27750, Loss 1.108107089996338\n",
            "Step 27775, Loss 1.0500510931015015\n",
            "Step 27800, Loss 0.9260188937187195\n",
            "Step 27825, Loss 0.8773024082183838\n",
            "Step 27850, Loss 0.9770769476890564\n",
            "Step 27875, Loss 1.058581829071045\n",
            "Step 27900, Loss 1.2508258819580078\n",
            "Step 27925, Loss 1.180957555770874\n",
            "Step 27950, Loss 1.1410599946975708\n",
            "Step 27975, Loss 0.9368127584457397\n",
            "Step 28000, Loss 1.3513379096984863\n",
            "Step 28025, Loss 1.0830193758010864\n",
            "Step 28050, Loss 1.2223641872406006\n",
            "Step 28075, Loss 1.1577420234680176\n",
            "Starting epoch 81/160, LR = [0.09999999999999912]\n",
            "Step 28100, Loss 1.0956155061721802\n",
            "Step 28125, Loss 1.1686303615570068\n",
            "Step 28150, Loss 0.9216377139091492\n",
            "Step 28175, Loss 1.1508197784423828\n",
            "Step 28200, Loss 0.9976899027824402\n",
            "Step 28225, Loss 1.0028783082962036\n",
            "Step 28250, Loss 1.0329374074935913\n",
            "Step 28275, Loss 0.9226558208465576\n",
            "Step 28300, Loss 1.173418402671814\n",
            "Step 28325, Loss 0.9164307713508606\n",
            "Step 28350, Loss 1.0117583274841309\n",
            "Step 28375, Loss 0.9344420433044434\n",
            "Step 28400, Loss 1.094939112663269\n",
            "Step 28425, Loss 1.0711206197738647\n",
            "Starting epoch 82/160, LR = [0.09938441702975602]\n",
            "Step 28450, Loss 0.869818925857544\n",
            "Step 28475, Loss 0.9904782772064209\n",
            "Step 28500, Loss 0.8578842878341675\n",
            "Step 28525, Loss 1.106231927871704\n",
            "Step 28550, Loss 0.938620388507843\n",
            "Step 28575, Loss 1.0891635417938232\n",
            "Step 28600, Loss 0.9791117906570435\n",
            "Step 28625, Loss 1.2175378799438477\n",
            "Step 28650, Loss 0.9759014844894409\n",
            "Step 28675, Loss 0.8637557029724121\n",
            "Step 28700, Loss 1.0334967374801636\n",
            "Step 28725, Loss 1.095397710800171\n",
            "Step 28750, Loss 1.2117016315460205\n",
            "Step 28775, Loss 1.0462892055511475\n",
            "Starting epoch 83/160, LR = [0.09755282581475685]\n",
            "Step 28800, Loss 0.9335736632347107\n",
            "Step 28825, Loss 0.674178421497345\n",
            "Step 28850, Loss 1.012590765953064\n",
            "Step 28875, Loss 1.0999443531036377\n",
            "Step 28900, Loss 0.8222149014472961\n",
            "Step 28925, Loss 0.7916266918182373\n",
            "Step 28950, Loss 0.9503084421157837\n",
            "Step 28975, Loss 1.122780203819275\n",
            "Step 29000, Loss 1.0671659708023071\n",
            "Step 29025, Loss 1.0657237768173218\n",
            "Step 29050, Loss 1.0558122396469116\n",
            "Step 29075, Loss 1.2290444374084473\n",
            "Step 29100, Loss 1.0530322790145874\n",
            "Step 29125, Loss 0.9992765784263611\n",
            "Starting epoch 84/160, LR = [0.09455032620941756]\n",
            "Step 29150, Loss 0.9460992217063904\n",
            "Step 29175, Loss 1.026983380317688\n",
            "Step 29200, Loss 0.717411994934082\n",
            "Step 29225, Loss 1.1539480686187744\n",
            "Step 29250, Loss 1.2213724851608276\n",
            "Step 29275, Loss 0.9320854544639587\n",
            "Step 29300, Loss 0.8776975870132446\n",
            "Step 29325, Loss 0.8637102842330933\n",
            "Step 29350, Loss 1.2335963249206543\n",
            "Step 29375, Loss 0.9891940355300903\n",
            "Step 29400, Loss 1.1343284845352173\n",
            "Step 29425, Loss 0.9867823123931885\n",
            "Step 29450, Loss 0.940742015838623\n",
            "Step 29475, Loss 0.919026792049408\n",
            "Starting epoch 85/160, LR = [0.09045084971874659]\n",
            "Step 29500, Loss 0.8383592367172241\n",
            "Step 29525, Loss 0.9005088210105896\n",
            "Step 29550, Loss 1.113311767578125\n",
            "Step 29575, Loss 0.925765872001648\n",
            "Step 29600, Loss 0.7166795134544373\n",
            "Step 29625, Loss 0.9026246070861816\n",
            "Step 29650, Loss 0.9899183511734009\n",
            "Step 29675, Loss 1.0399010181427002\n",
            "Step 29700, Loss 1.0534300804138184\n",
            "Step 29725, Loss 0.8484848141670227\n",
            "Step 29750, Loss 1.0657497644424438\n",
            "Step 29775, Loss 0.8791995644569397\n",
            "Step 29800, Loss 1.2190781831741333\n",
            "Step 29825, Loss 0.9344568848609924\n",
            "Starting epoch 86/160, LR = [0.08535533905932662]\n",
            "Step 29850, Loss 0.8219525218009949\n",
            "Step 29875, Loss 0.8773702383041382\n",
            "Step 29900, Loss 0.8514413833618164\n",
            "Step 29925, Loss 0.7503499984741211\n",
            "Step 29950, Loss 0.9305953979492188\n",
            "Step 29975, Loss 0.8907185196876526\n",
            "Step 30000, Loss 0.8921587467193604\n",
            "Step 30025, Loss 0.9334136247634888\n",
            "Step 30050, Loss 1.031267762184143\n",
            "Step 30075, Loss 0.9618650674819946\n",
            "Step 30100, Loss 1.0963077545166016\n",
            "Step 30125, Loss 0.9773586988449097\n",
            "Step 30150, Loss 1.1262730360031128\n",
            "Step 30175, Loss 0.863915741443634\n",
            "Starting epoch 87/160, LR = [0.07938926261462298]\n",
            "Step 30200, Loss 0.9428138732910156\n",
            "Step 30225, Loss 1.0273562669754028\n",
            "Step 30250, Loss 0.8246889710426331\n",
            "Step 30275, Loss 0.9168202877044678\n",
            "Step 30300, Loss 1.1124231815338135\n",
            "Step 30325, Loss 0.871128499507904\n",
            "Step 30350, Loss 0.7485558986663818\n",
            "Step 30375, Loss 0.839165449142456\n",
            "Step 30400, Loss 1.06983482837677\n",
            "Step 30425, Loss 0.716055691242218\n",
            "Step 30450, Loss 0.8383228182792664\n",
            "Step 30475, Loss 1.2216006517410278\n",
            "Step 30500, Loss 0.9793440699577332\n",
            "Step 30525, Loss 0.7934993505477905\n",
            "Starting epoch 88/160, LR = [0.07269952498697677]\n",
            "Step 30550, Loss 0.7715685963630676\n",
            "Step 30575, Loss 0.6783285737037659\n",
            "Step 30600, Loss 0.9538882970809937\n",
            "Step 30625, Loss 0.9146209955215454\n",
            "Step 30650, Loss 0.8047485947608948\n",
            "Step 30675, Loss 1.0434165000915527\n",
            "Step 30700, Loss 0.8090153932571411\n",
            "Step 30725, Loss 0.8296045660972595\n",
            "Step 30750, Loss 0.7992856502532959\n",
            "Step 30775, Loss 0.7775903940200806\n",
            "Step 30800, Loss 0.7980085015296936\n",
            "Step 30825, Loss 0.8036346435546875\n",
            "Step 30850, Loss 0.7656869888305664\n",
            "Step 30875, Loss 1.0067919492721558\n",
            "Starting epoch 89/160, LR = [0.06545084971874691]\n",
            "Step 30900, Loss 0.8006022572517395\n",
            "Step 30925, Loss 0.909448504447937\n",
            "Step 30950, Loss 0.7325116395950317\n",
            "Step 30975, Loss 0.7037529349327087\n",
            "Step 31000, Loss 0.9212327599525452\n",
            "Step 31025, Loss 0.5507376194000244\n",
            "Step 31050, Loss 0.8593094348907471\n",
            "Step 31075, Loss 0.8351420760154724\n",
            "Step 31100, Loss 0.8181005716323853\n",
            "Step 31125, Loss 0.7139966487884521\n",
            "Step 31150, Loss 0.7852489948272705\n",
            "Step 31175, Loss 0.8738878965377808\n",
            "Step 31200, Loss 1.0338698625564575\n",
            "Step 31225, Loss 0.5841461420059204\n",
            "Starting epoch 90/160, LR = [0.05782172325201102]\n",
            "Step 31250, Loss 0.6571113467216492\n",
            "Step 31275, Loss 0.702675461769104\n",
            "Step 31300, Loss 0.7055299282073975\n",
            "Step 31325, Loss 0.7866657376289368\n",
            "Step 31350, Loss 0.5934221744537354\n",
            "Step 31375, Loss 0.7870429158210754\n",
            "Step 31400, Loss 0.7712260484695435\n",
            "Step 31425, Loss 0.6929102540016174\n",
            "Step 31450, Loss 0.8244925737380981\n",
            "Step 31475, Loss 0.680561363697052\n",
            "Step 31500, Loss 0.6723489165306091\n",
            "Step 31525, Loss 0.8681439757347107\n",
            "Step 31550, Loss 0.5978521704673767\n",
            "Step 31575, Loss 0.7120672464370728\n",
            "Starting epoch 91/160, LR = [0.049999999999999586]\n",
            "Step 31600, Loss 0.8176631331443787\n",
            "Step 31625, Loss 0.7296736836433411\n",
            "Step 31650, Loss 0.6908349394798279\n",
            "Step 31675, Loss 0.6936290860176086\n",
            "Step 31700, Loss 0.9881104230880737\n",
            "Step 31725, Loss 0.8030796051025391\n",
            "Step 31750, Loss 0.8285622596740723\n",
            "Step 31775, Loss 0.7602801322937012\n",
            "Step 31800, Loss 0.7471868991851807\n",
            "Step 31825, Loss 0.718445360660553\n",
            "Step 31850, Loss 0.7372370958328247\n",
            "Step 31875, Loss 0.6837022304534912\n",
            "Step 31900, Loss 0.7120157480239868\n",
            "Step 31925, Loss 0.777265191078186\n",
            "Starting epoch 92/160, LR = [0.042178276747988165]\n",
            "Step 31950, Loss 0.5806788206100464\n",
            "Step 31975, Loss 0.5276404023170471\n",
            "Step 32000, Loss 0.48599982261657715\n",
            "Step 32025, Loss 0.5627952218055725\n",
            "Step 32050, Loss 0.7507871389389038\n",
            "Step 32075, Loss 0.6733328700065613\n",
            "Step 32100, Loss 0.6000809669494629\n",
            "Step 32125, Loss 0.5510484576225281\n",
            "Step 32150, Loss 0.6110170483589172\n",
            "Step 32175, Loss 0.6296493411064148\n",
            "Step 32200, Loss 0.7231762409210205\n",
            "Step 32225, Loss 0.6331505179405212\n",
            "Step 32250, Loss 0.6648423075675964\n",
            "Step 32275, Loss 0.6054604053497314\n",
            "Starting epoch 93/160, LR = [0.034549150281252355]\n",
            "Step 32300, Loss 0.5460698008537292\n",
            "Step 32325, Loss 0.6905286312103271\n",
            "Step 32350, Loss 0.418237566947937\n",
            "Step 32375, Loss 0.6704919338226318\n",
            "Step 32400, Loss 0.6328569054603577\n",
            "Step 32425, Loss 0.4683082699775696\n",
            "Step 32450, Loss 0.5190245509147644\n",
            "Step 32475, Loss 0.709181547164917\n",
            "Step 32500, Loss 0.5190315842628479\n",
            "Step 32525, Loss 0.6327552199363708\n",
            "Step 32550, Loss 0.5305756330490112\n",
            "Step 32575, Loss 0.6248701810836792\n",
            "Step 32600, Loss 0.7286378145217896\n",
            "Step 32625, Loss 0.6102800369262695\n",
            "Starting epoch 94/160, LR = [0.027300475013022494]\n",
            "Step 32650, Loss 0.5375725030899048\n",
            "Step 32675, Loss 0.6581600904464722\n",
            "Step 32700, Loss 0.5159688591957092\n",
            "Step 32725, Loss 0.5073878765106201\n",
            "Step 32750, Loss 0.416644811630249\n",
            "Step 32775, Loss 0.4565030336380005\n",
            "Step 32800, Loss 0.4760817587375641\n",
            "Step 32825, Loss 0.47786572575569153\n",
            "Step 32850, Loss 0.5481007695198059\n",
            "Step 32875, Loss 0.4197233021259308\n",
            "Step 32900, Loss 0.694001317024231\n",
            "Step 32925, Loss 0.8121175765991211\n",
            "Step 32950, Loss 0.6000059843063354\n",
            "Step 32975, Loss 0.5160727500915527\n",
            "Starting epoch 95/160, LR = [0.020610737385376117]\n",
            "Step 33000, Loss 0.5256237387657166\n",
            "Step 33025, Loss 0.44084206223487854\n",
            "Step 33050, Loss 0.42858779430389404\n",
            "Step 33075, Loss 0.4713555574417114\n",
            "Step 33100, Loss 0.4433402419090271\n",
            "Step 33125, Loss 0.4219808578491211\n",
            "Step 33150, Loss 0.3463729918003082\n",
            "Step 33175, Loss 0.5239202976226807\n",
            "Step 33200, Loss 0.4141557216644287\n",
            "Step 33225, Loss 0.5513714551925659\n",
            "Step 33250, Loss 0.552420437335968\n",
            "Step 33275, Loss 0.42440012097358704\n",
            "Step 33300, Loss 0.4851360023021698\n",
            "Step 33325, Loss 0.4055994749069214\n",
            "Starting epoch 96/160, LR = [0.014644660940672485]\n",
            "Step 33350, Loss 0.45949065685272217\n",
            "Step 33375, Loss 0.4414482116699219\n",
            "Step 33400, Loss 0.3492893576622009\n",
            "Step 33425, Loss 0.40836912393569946\n",
            "Step 33450, Loss 0.3663368225097656\n",
            "Step 33475, Loss 0.5521594285964966\n",
            "Step 33500, Loss 0.3906940221786499\n",
            "Step 33525, Loss 0.4527595341205597\n",
            "Step 33550, Loss 0.542640745639801\n",
            "Step 33575, Loss 0.37582364678382874\n",
            "Step 33600, Loss 0.35677647590637207\n",
            "Step 33625, Loss 0.31487923860549927\n",
            "Step 33650, Loss 0.33311259746551514\n",
            "Step 33675, Loss 0.5271995663642883\n",
            "Starting epoch 97/160, LR = [0.009549150281252562]\n",
            "Step 33700, Loss 0.4703987240791321\n",
            "Step 33725, Loss 0.4390701651573181\n",
            "Step 33750, Loss 0.38029834628105164\n",
            "Step 33775, Loss 0.3701601028442383\n",
            "Step 33800, Loss 0.4999884366989136\n",
            "Step 33825, Loss 0.3676466941833496\n",
            "Step 33850, Loss 0.3651387691497803\n",
            "Step 33875, Loss 0.30723708868026733\n",
            "Step 33900, Loss 0.42215821146965027\n",
            "Step 33925, Loss 0.3340680003166199\n",
            "Step 33950, Loss 0.33270832896232605\n",
            "Step 33975, Loss 0.23069524765014648\n",
            "Step 34000, Loss 0.43480807542800903\n",
            "Step 34025, Loss 0.46916353702545166\n",
            "Starting epoch 98/160, LR = [0.005449673790581592]\n",
            "Step 34050, Loss 0.3546096384525299\n",
            "Step 34075, Loss 0.2777783274650574\n",
            "Step 34100, Loss 0.4003508687019348\n",
            "Step 34125, Loss 0.43597984313964844\n",
            "Step 34150, Loss 0.44748654961586\n",
            "Step 34175, Loss 0.4781098961830139\n",
            "Step 34200, Loss 0.3028832972049713\n",
            "Step 34225, Loss 0.3360929489135742\n",
            "Step 34250, Loss 0.4536130428314209\n",
            "Step 34275, Loss 0.44259750843048096\n",
            "Step 34300, Loss 0.4207949936389923\n",
            "Step 34325, Loss 0.23470370471477509\n",
            "Step 34350, Loss 0.4553253650665283\n",
            "Step 34375, Loss 0.3057060241699219\n",
            "Starting epoch 99/160, LR = [0.002447174185242336]\n",
            "Step 34400, Loss 0.3780824840068817\n",
            "Step 34425, Loss 0.4040721356868744\n",
            "Step 34450, Loss 0.27846604585647583\n",
            "Step 34475, Loss 0.479854017496109\n",
            "Step 34500, Loss 0.28799551725387573\n",
            "Step 34525, Loss 0.38557571172714233\n",
            "Step 34550, Loss 0.39033007621765137\n",
            "Step 34575, Loss 0.34815603494644165\n",
            "Step 34600, Loss 0.3441064655780792\n",
            "Step 34625, Loss 0.37563157081604004\n",
            "Step 34650, Loss 0.29531651735305786\n",
            "Step 34675, Loss 0.49408379197120667\n",
            "Step 34700, Loss 0.3319789469242096\n",
            "Step 34725, Loss 0.2982393801212311\n",
            "Starting epoch 100/160, LR = [0.0006155829702431063]\n",
            "Step 34750, Loss 0.2865760624408722\n",
            "Step 34775, Loss 0.34189069271087646\n",
            "Step 34800, Loss 0.3406781256198883\n",
            "Step 34825, Loss 0.38541722297668457\n",
            "Step 34850, Loss 0.4414899945259094\n",
            "Step 34875, Loss 0.3911726474761963\n",
            "Step 34900, Loss 0.3157365918159485\n",
            "Step 34925, Loss 0.33121252059936523\n",
            "Step 34950, Loss 0.29080280661582947\n",
            "Step 34975, Loss 0.34567809104919434\n",
            "Step 35000, Loss 0.29795703291893005\n",
            "Step 35025, Loss 0.36883020401000977\n",
            "Step 35050, Loss 0.4689251184463501\n",
            "Step 35075, Loss 0.3202776610851288\n",
            "Starting epoch 101/160, LR = [0.0]\n",
            "Step 35100, Loss 0.2788035273551941\n",
            "Step 35125, Loss 0.4488208293914795\n",
            "Step 35150, Loss 0.34190741181373596\n",
            "Step 35175, Loss 0.3100486993789673\n",
            "Step 35200, Loss 0.3008453845977783\n",
            "Step 35225, Loss 0.43573278188705444\n",
            "Step 35250, Loss 0.34137025475502014\n",
            "Step 35275, Loss 0.31123244762420654\n",
            "Step 35300, Loss 0.36865437030792236\n",
            "Step 35325, Loss 0.27380216121673584\n",
            "Step 35350, Loss 0.3450051546096802\n",
            "Step 35375, Loss 0.35497716069221497\n",
            "Step 35400, Loss 0.21230140328407288\n",
            "Step 35425, Loss 0.4115397334098816\n",
            "Step 35450, Loss 0.42754435539245605\n",
            "Starting epoch 102/160, LR = [0.0006155829702431115]\n",
            "Step 35475, Loss 0.382205069065094\n",
            "Step 35500, Loss 0.4287634789943695\n",
            "Step 35525, Loss 0.3260670304298401\n",
            "Step 35550, Loss 0.3060421943664551\n",
            "Step 35575, Loss 0.3101758360862732\n",
            "Step 35600, Loss 0.2817871868610382\n",
            "Step 35625, Loss 0.5031829476356506\n",
            "Step 35650, Loss 0.4628492593765259\n",
            "Step 35675, Loss 0.26354682445526123\n",
            "Step 35700, Loss 0.39386188983917236\n",
            "Step 35725, Loss 0.36937248706817627\n",
            "Step 35750, Loss 0.3955778479576111\n",
            "Step 35775, Loss 0.3792452812194824\n",
            "Step 35800, Loss 0.370300829410553\n",
            "Starting epoch 103/160, LR = [0.002447174185242263]\n",
            "Step 35825, Loss 0.249410018324852\n",
            "Step 35850, Loss 0.3627520501613617\n",
            "Step 35875, Loss 0.36229175329208374\n",
            "Step 35900, Loss 0.33772075176239014\n",
            "Step 35925, Loss 0.42226412892341614\n",
            "Step 35950, Loss 0.2765166163444519\n",
            "Step 35975, Loss 0.31836557388305664\n",
            "Step 36000, Loss 0.41312408447265625\n",
            "Step 36025, Loss 0.3438728153705597\n",
            "Step 36050, Loss 0.4137653112411499\n",
            "Step 36075, Loss 0.3044707179069519\n",
            "Step 36100, Loss 0.2675795555114746\n",
            "Step 36125, Loss 0.3494119346141815\n",
            "Step 36150, Loss 0.265855997800827\n",
            "Starting epoch 104/160, LR = [0.0054496737905815225]\n",
            "Step 36175, Loss 0.26517587900161743\n",
            "Step 36200, Loss 0.22931820154190063\n",
            "Step 36225, Loss 0.29829275608062744\n",
            "Step 36250, Loss 0.2916819155216217\n",
            "Step 36275, Loss 0.3154943287372589\n",
            "Step 36300, Loss 0.4145149290561676\n",
            "Step 36325, Loss 0.3198031187057495\n",
            "Step 36350, Loss 0.34868425130844116\n",
            "Step 36375, Loss 0.3305622935295105\n",
            "Step 36400, Loss 0.3341665267944336\n",
            "Step 36425, Loss 0.3342234492301941\n",
            "Step 36450, Loss 0.37825173139572144\n",
            "Step 36475, Loss 0.3443080186843872\n",
            "Step 36500, Loss 0.42672276496887207\n",
            "Starting epoch 105/160, LR = [0.009549150281252524]\n",
            "Step 36525, Loss 0.3826906085014343\n",
            "Step 36550, Loss 0.36568307876586914\n",
            "Step 36575, Loss 0.20953765511512756\n",
            "Step 36600, Loss 0.2742277979850769\n",
            "Step 36625, Loss 0.48285138607025146\n",
            "Step 36650, Loss 0.30405622720718384\n",
            "Step 36675, Loss 0.21825146675109863\n",
            "Step 36700, Loss 0.46697670221328735\n",
            "Step 36725, Loss 0.30032384395599365\n",
            "Step 36750, Loss 0.41982150077819824\n",
            "Step 36775, Loss 0.34770840406417847\n",
            "Step 36800, Loss 0.40426310896873474\n",
            "Step 36825, Loss 0.490943968296051\n",
            "Step 36850, Loss 0.29386642575263977\n",
            "Starting epoch 106/160, LR = [0.014644660940672499]\n",
            "Step 36875, Loss 0.4140826463699341\n",
            "Step 36900, Loss 0.49615418910980225\n",
            "Step 36925, Loss 0.3905722498893738\n",
            "Step 36950, Loss 0.44172829389572144\n",
            "Step 36975, Loss 0.41158246994018555\n",
            "Step 37000, Loss 0.39664533734321594\n",
            "Step 37025, Loss 0.3974887728691101\n",
            "Step 37050, Loss 0.4742256999015808\n",
            "Step 37075, Loss 0.37829136848449707\n",
            "Step 37100, Loss 0.2995750308036804\n",
            "Step 37125, Loss 0.3761720359325409\n",
            "Step 37150, Loss 0.33837616443634033\n",
            "Step 37175, Loss 0.32209306955337524\n",
            "Step 37200, Loss 0.4262935221195221\n",
            "Starting epoch 107/160, LR = [0.020610737385376197]\n",
            "Step 37225, Loss 0.592581033706665\n",
            "Step 37250, Loss 0.40757277607917786\n",
            "Step 37275, Loss 0.39836573600769043\n",
            "Step 37300, Loss 0.35848912596702576\n",
            "Step 37325, Loss 0.34303683042526245\n",
            "Step 37350, Loss 0.43233197927474976\n",
            "Step 37375, Loss 0.4298657774925232\n",
            "Step 37400, Loss 0.48945748805999756\n",
            "Step 37425, Loss 0.375857412815094\n",
            "Step 37450, Loss 0.6015312671661377\n",
            "Step 37475, Loss 0.558580756187439\n",
            "Step 37500, Loss 0.5108324885368347\n",
            "Step 37525, Loss 0.533302903175354\n",
            "Step 37550, Loss 0.3402591347694397\n",
            "Starting epoch 108/160, LR = [0.027300475013022338]\n",
            "Step 37575, Loss 0.528500497341156\n",
            "Step 37600, Loss 0.5179316401481628\n",
            "Step 37625, Loss 0.4357326626777649\n",
            "Step 37650, Loss 0.6100765466690063\n",
            "Step 37675, Loss 0.5786183476448059\n",
            "Step 37700, Loss 0.37182989716529846\n",
            "Step 37725, Loss 0.4216815233230591\n",
            "Step 37750, Loss 0.5395895838737488\n",
            "Step 37775, Loss 0.49721068143844604\n",
            "Step 37800, Loss 0.47638624906539917\n",
            "Step 37825, Loss 0.554176926612854\n",
            "Step 37850, Loss 0.4807533621788025\n",
            "Step 37875, Loss 0.5195937752723694\n",
            "Step 37900, Loss 0.6679272055625916\n",
            "Starting epoch 109/160, LR = [0.03454915028125227]\n",
            "Step 37925, Loss 0.4429113566875458\n",
            "Step 37950, Loss 0.5900281667709351\n",
            "Step 37975, Loss 0.586143970489502\n",
            "Step 38000, Loss 0.6265708804130554\n",
            "Step 38025, Loss 0.48811832070350647\n",
            "Step 38050, Loss 0.5001816153526306\n",
            "Step 38075, Loss 0.5977751612663269\n",
            "Step 38100, Loss 0.5658842325210571\n",
            "Step 38125, Loss 0.5411370992660522\n",
            "Step 38150, Loss 0.6572719216346741\n",
            "Step 38175, Loss 0.4782412350177765\n",
            "Step 38200, Loss 0.5353704690933228\n",
            "Step 38225, Loss 0.8417972326278687\n",
            "Step 38250, Loss 0.6728235483169556\n",
            "Starting epoch 110/160, LR = [0.0421782767479879]\n",
            "Step 38275, Loss 0.5458626747131348\n",
            "Step 38300, Loss 0.7212382555007935\n",
            "Step 38325, Loss 0.6413692831993103\n",
            "Step 38350, Loss 0.8096698522567749\n",
            "Step 38375, Loss 0.622157096862793\n",
            "Step 38400, Loss 0.6231342554092407\n",
            "Step 38425, Loss 0.8888965845108032\n",
            "Step 38450, Loss 0.5361967086791992\n",
            "Step 38475, Loss 0.6436065435409546\n",
            "Step 38500, Loss 0.773496150970459\n",
            "Step 38525, Loss 0.5526704788208008\n",
            "Step 38550, Loss 0.6124000549316406\n",
            "Step 38575, Loss 0.8784586787223816\n",
            "Step 38600, Loss 0.6636027097702026\n",
            "Starting epoch 111/160, LR = [0.04999999999999959]\n",
            "Step 38625, Loss 0.5221825242042542\n",
            "Step 38650, Loss 0.8238330483436584\n",
            "Step 38675, Loss 0.707778811454773\n",
            "Step 38700, Loss 0.8155685067176819\n",
            "Step 38725, Loss 0.744538426399231\n",
            "Step 38750, Loss 0.9763138890266418\n",
            "Step 38775, Loss 0.8115453124046326\n",
            "Step 38800, Loss 0.7928979396820068\n",
            "Step 38825, Loss 0.724195659160614\n",
            "Step 38850, Loss 0.5196695923805237\n",
            "Step 38875, Loss 0.7846500873565674\n",
            "Step 38900, Loss 0.7549203038215637\n",
            "Step 38925, Loss 0.7225778698921204\n",
            "Step 38950, Loss 0.7745835185050964\n",
            "Starting epoch 112/160, LR = [0.05782172325201093]\n",
            "Step 38975, Loss 0.6651895046234131\n",
            "Step 39000, Loss 0.7479364275932312\n",
            "Step 39025, Loss 0.8729479312896729\n",
            "Step 39050, Loss 0.7194887399673462\n",
            "Step 39075, Loss 0.8166624903678894\n",
            "Step 39100, Loss 1.131942629814148\n",
            "Step 39125, Loss 0.998884379863739\n",
            "Step 39150, Loss 0.9438938498497009\n",
            "Step 39175, Loss 0.7424826622009277\n",
            "Step 39200, Loss 0.8560048341751099\n",
            "Step 39225, Loss 0.9063277244567871\n",
            "Step 39250, Loss 0.8820537328720093\n",
            "Step 39275, Loss 0.8589709997177124\n",
            "Step 39300, Loss 0.8511869311332703\n",
            "Starting epoch 113/160, LR = [0.06545084971874673]\n",
            "Step 39325, Loss 0.7690650820732117\n",
            "Step 39350, Loss 0.6582309007644653\n",
            "Step 39375, Loss 0.8312745690345764\n",
            "Step 39400, Loss 0.9037206768989563\n",
            "Step 39425, Loss 1.0065704584121704\n",
            "Step 39450, Loss 1.0033957958221436\n",
            "Step 39475, Loss 0.9606587886810303\n",
            "Step 39500, Loss 0.7688907384872437\n",
            "Step 39525, Loss 1.0094311237335205\n",
            "Step 39550, Loss 1.0014631748199463\n",
            "Step 39575, Loss 0.818234384059906\n",
            "Step 39600, Loss 0.8507341146469116\n",
            "Step 39625, Loss 0.9511319994926453\n",
            "Step 39650, Loss 0.9289585947990417\n",
            "Starting epoch 114/160, LR = [0.07269952498697668]\n",
            "Step 39675, Loss 0.8332800269126892\n",
            "Step 39700, Loss 0.702226996421814\n",
            "Step 39725, Loss 0.9084262251853943\n",
            "Step 39750, Loss 0.7870362401008606\n",
            "Step 39775, Loss 0.7932586669921875\n",
            "Step 39800, Loss 0.9431291222572327\n",
            "Step 39825, Loss 1.0784616470336914\n",
            "Step 39850, Loss 0.789794385433197\n",
            "Step 39875, Loss 0.8623254299163818\n",
            "Step 39900, Loss 0.7635335922241211\n",
            "Step 39925, Loss 0.9775227308273315\n",
            "Step 39950, Loss 1.1746273040771484\n",
            "Step 39975, Loss 1.0848278999328613\n",
            "Step 40000, Loss 0.7846801280975342\n",
            "Starting epoch 115/160, LR = [0.07938926261462283]\n",
            "Step 40025, Loss 0.9203482866287231\n",
            "Step 40050, Loss 0.9765185117721558\n",
            "Step 40075, Loss 0.9056093096733093\n",
            "Step 40100, Loss 1.1445748805999756\n",
            "Step 40125, Loss 0.9143421649932861\n",
            "Step 40150, Loss 1.0052651166915894\n",
            "Step 40175, Loss 0.727746844291687\n",
            "Step 40200, Loss 0.8408157229423523\n",
            "Step 40225, Loss 1.2231314182281494\n",
            "Step 40250, Loss 1.0438899993896484\n",
            "Step 40275, Loss 1.1334782838821411\n",
            "Step 40300, Loss 0.8723451495170593\n",
            "Step 40325, Loss 1.2062077522277832\n",
            "Step 40350, Loss 1.085410237312317\n",
            "Starting epoch 116/160, LR = [0.08535533905932666]\n",
            "Step 40375, Loss 1.0034122467041016\n",
            "Step 40400, Loss 1.0288654565811157\n",
            "Step 40425, Loss 0.8299433588981628\n",
            "Step 40450, Loss 1.0202515125274658\n",
            "Step 40475, Loss 1.1108640432357788\n",
            "Step 40500, Loss 1.1226905584335327\n",
            "Step 40525, Loss 1.1573983430862427\n",
            "Step 40550, Loss 0.950940728187561\n",
            "Step 40575, Loss 1.2484424114227295\n",
            "Step 40600, Loss 0.86686772108078\n",
            "Step 40625, Loss 0.9439670443534851\n",
            "Step 40650, Loss 1.0200107097625732\n",
            "Step 40675, Loss 1.105434775352478\n",
            "Step 40700, Loss 1.3051398992538452\n",
            "Starting epoch 117/160, LR = [0.09045084971874652]\n",
            "Step 40725, Loss 0.7947731018066406\n",
            "Step 40750, Loss 0.8140032887458801\n",
            "Step 40775, Loss 1.241463303565979\n",
            "Step 40800, Loss 1.1334244012832642\n",
            "Step 40825, Loss 0.8202353119850159\n",
            "Step 40850, Loss 0.7977371215820312\n",
            "Step 40875, Loss 1.0360463857650757\n",
            "Step 40900, Loss 0.9859374165534973\n",
            "Step 40925, Loss 0.9338186979293823\n",
            "Step 40950, Loss 0.9507575035095215\n",
            "Step 40975, Loss 1.1417146921157837\n",
            "Step 41000, Loss 0.8369386196136475\n",
            "Step 41025, Loss 1.1138743162155151\n",
            "Step 41050, Loss 1.1320569515228271\n",
            "Starting epoch 118/160, LR = [0.09455032620941752]\n",
            "Step 41075, Loss 0.9347382187843323\n",
            "Step 41100, Loss 1.074292540550232\n",
            "Step 41125, Loss 1.143235445022583\n",
            "Step 41150, Loss 1.1312248706817627\n",
            "Step 41175, Loss 0.9311790466308594\n",
            "Step 41200, Loss 1.2125743627548218\n",
            "Step 41225, Loss 1.0634366273880005\n",
            "Step 41250, Loss 1.0158941745758057\n",
            "Step 41275, Loss 1.3247944116592407\n",
            "Step 41300, Loss 0.9424842596054077\n",
            "Step 41325, Loss 0.9567835330963135\n",
            "Step 41350, Loss 1.0168606042861938\n",
            "Step 41375, Loss 1.0495134592056274\n",
            "Step 41400, Loss 0.9139846563339233\n",
            "Starting epoch 119/160, LR = [0.09755282581475674]\n",
            "Step 41425, Loss 0.8083601593971252\n",
            "Step 41450, Loss 0.9384512901306152\n",
            "Step 41475, Loss 1.0863569974899292\n",
            "Step 41500, Loss 1.0704591274261475\n",
            "Step 41525, Loss 0.9427483081817627\n",
            "Step 41550, Loss 0.9135675430297852\n",
            "Step 41575, Loss 1.2657262086868286\n",
            "Step 41600, Loss 1.2291189432144165\n",
            "Step 41625, Loss 0.85350501537323\n",
            "Step 41650, Loss 1.0979535579681396\n",
            "Step 41675, Loss 1.2777602672576904\n",
            "Step 41700, Loss 1.080627679824829\n",
            "Step 41725, Loss 0.8401675224304199\n",
            "Step 41750, Loss 1.1265281438827515\n",
            "Starting epoch 120/160, LR = [0.09938441702975596]\n",
            "Step 41775, Loss 1.0572314262390137\n",
            "Step 41800, Loss 0.9962965846061707\n",
            "Step 41825, Loss 1.1643985509872437\n",
            "Step 41850, Loss 1.017305850982666\n",
            "Step 41875, Loss 1.100672960281372\n",
            "Step 41900, Loss 1.2154219150543213\n",
            "Step 41925, Loss 0.9199216365814209\n",
            "Step 41950, Loss 1.2341920137405396\n",
            "Step 41975, Loss 1.0920571088790894\n",
            "Step 42000, Loss 0.9171929359436035\n",
            "Step 42025, Loss 1.164634108543396\n",
            "Step 42050, Loss 1.258571982383728\n",
            "Step 42075, Loss 0.9311954379081726\n",
            "Step 42100, Loss 1.1442761421203613\n",
            "Starting epoch 121/160, LR = [0.0999999999999991]\n",
            "Step 42125, Loss 0.842431902885437\n",
            "Step 42150, Loss 1.0434448719024658\n",
            "Step 42175, Loss 1.1377540826797485\n",
            "Step 42200, Loss 1.1705172061920166\n",
            "Step 42225, Loss 1.1832821369171143\n",
            "Step 42250, Loss 1.1563682556152344\n",
            "Step 42275, Loss 1.141996145248413\n",
            "Step 42300, Loss 0.973373293876648\n",
            "Step 42325, Loss 1.055355191230774\n",
            "Step 42350, Loss 0.8469501733779907\n",
            "Step 42375, Loss 1.0361424684524536\n",
            "Step 42400, Loss 1.2517619132995605\n",
            "Step 42425, Loss 1.2959632873535156\n",
            "Step 42450, Loss 0.9887616038322449\n",
            "Starting epoch 122/160, LR = [0.09938441702975599]\n",
            "Step 42475, Loss 1.0953220129013062\n",
            "Step 42500, Loss 1.1415317058563232\n",
            "Step 42525, Loss 0.9503676295280457\n",
            "Step 42550, Loss 0.988364577293396\n",
            "Step 42575, Loss 0.9808037281036377\n",
            "Step 42600, Loss 1.1399731636047363\n",
            "Step 42625, Loss 1.1141687631607056\n",
            "Step 42650, Loss 1.1820932626724243\n",
            "Step 42675, Loss 0.9846658110618591\n",
            "Step 42700, Loss 1.0465643405914307\n",
            "Step 42725, Loss 0.9668046832084656\n",
            "Step 42750, Loss 0.9810574650764465\n",
            "Step 42775, Loss 0.7174959778785706\n",
            "Step 42800, Loss 1.0485341548919678\n",
            "Starting epoch 123/160, LR = [0.09755282581475679]\n",
            "Step 42825, Loss 0.9746108651161194\n",
            "Step 42850, Loss 1.0607136487960815\n",
            "Step 42875, Loss 1.1159677505493164\n",
            "Step 42900, Loss 0.7237893342971802\n",
            "Step 42925, Loss 1.0058468580245972\n",
            "Step 42950, Loss 0.8516127467155457\n",
            "Step 42975, Loss 0.8109805583953857\n",
            "Step 43000, Loss 0.9039402604103088\n",
            "Step 43025, Loss 1.0673516988754272\n",
            "Step 43050, Loss 1.0530155897140503\n",
            "Step 43075, Loss 1.0236395597457886\n",
            "Step 43100, Loss 1.0034126043319702\n",
            "Step 43125, Loss 1.0514757633209229\n",
            "Step 43150, Loss 1.0813571214675903\n",
            "Starting epoch 124/160, LR = [0.09455032620941758]\n",
            "Step 43175, Loss 0.9817905426025391\n",
            "Step 43200, Loss 0.9145504832267761\n",
            "Step 43225, Loss 0.9770808219909668\n",
            "Step 43250, Loss 0.763761043548584\n",
            "Step 43275, Loss 0.8942638039588928\n",
            "Step 43300, Loss 0.9169648885726929\n",
            "Step 43325, Loss 1.1557261943817139\n",
            "Step 43350, Loss 1.0954334735870361\n",
            "Step 43375, Loss 1.2366083860397339\n",
            "Step 43400, Loss 1.1802411079406738\n",
            "Step 43425, Loss 0.9964209198951721\n",
            "Step 43450, Loss 0.8329432606697083\n",
            "Step 43475, Loss 0.9592930674552917\n",
            "Step 43500, Loss 1.2046148777008057\n",
            "Starting epoch 125/160, LR = [0.09045084971874659]\n",
            "Step 43525, Loss 0.7404229044914246\n",
            "Step 43550, Loss 0.9573430418968201\n",
            "Step 43575, Loss 0.8591904640197754\n",
            "Step 43600, Loss 0.9107042551040649\n",
            "Step 43625, Loss 0.9475412368774414\n",
            "Step 43650, Loss 0.9856758713722229\n",
            "Step 43675, Loss 1.06626296043396\n",
            "Step 43700, Loss 1.0995792150497437\n",
            "Step 43725, Loss 0.8289394378662109\n",
            "Step 43750, Loss 0.9293153882026672\n",
            "Step 43775, Loss 0.8898085951805115\n",
            "Step 43800, Loss 0.961801290512085\n",
            "Step 43825, Loss 1.2452232837677002\n",
            "Step 43850, Loss 0.9453291893005371\n",
            "Starting epoch 126/160, LR = [0.08535533905932674]\n",
            "Step 43875, Loss 1.0146814584732056\n",
            "Step 43900, Loss 1.1025419235229492\n",
            "Step 43925, Loss 1.105073094367981\n",
            "Step 43950, Loss 0.7990030646324158\n",
            "Step 43975, Loss 0.9510418176651001\n",
            "Step 44000, Loss 0.8863692283630371\n",
            "Step 44025, Loss 0.7306399941444397\n",
            "Step 44050, Loss 0.9067845940589905\n",
            "Step 44075, Loss 0.975953221321106\n",
            "Step 44100, Loss 0.8736586570739746\n",
            "Step 44125, Loss 0.9930400848388672\n",
            "Step 44150, Loss 0.9043566584587097\n",
            "Step 44175, Loss 1.314247727394104\n",
            "Step 44200, Loss 0.9318341016769409\n",
            "Step 44225, Loss 1.123607873916626\n",
            "Starting epoch 127/160, LR = [0.07938926261462292]\n",
            "Step 44250, Loss 0.8256120681762695\n",
            "Step 44275, Loss 0.858339786529541\n",
            "Step 44300, Loss 0.8869191408157349\n",
            "Step 44325, Loss 1.0028719902038574\n",
            "Step 44350, Loss 0.8399757742881775\n",
            "Step 44375, Loss 0.8676086068153381\n",
            "Step 44400, Loss 0.9538325071334839\n",
            "Step 44425, Loss 1.0122860670089722\n",
            "Step 44450, Loss 0.7698085904121399\n",
            "Step 44475, Loss 0.748335063457489\n",
            "Step 44500, Loss 0.803792417049408\n",
            "Step 44525, Loss 1.203916311264038\n",
            "Step 44550, Loss 0.7713766098022461\n",
            "Step 44575, Loss 0.6357660293579102\n",
            "Starting epoch 128/160, LR = [0.07269952498697678]\n",
            "Step 44600, Loss 0.6870378851890564\n",
            "Step 44625, Loss 0.8578348755836487\n",
            "Step 44650, Loss 1.0637590885162354\n",
            "Step 44675, Loss 1.079220175743103\n",
            "Step 44700, Loss 1.0192115306854248\n",
            "Step 44725, Loss 0.9135537147521973\n",
            "Step 44750, Loss 0.8018820881843567\n",
            "Step 44775, Loss 0.7478384971618652\n",
            "Step 44800, Loss 0.8388985395431519\n",
            "Step 44825, Loss 0.9741248488426208\n",
            "Step 44850, Loss 1.090238094329834\n",
            "Step 44875, Loss 0.9242379665374756\n",
            "Step 44900, Loss 1.0074560642242432\n",
            "Step 44925, Loss 0.8755898475646973\n",
            "Starting epoch 129/160, LR = [0.06545084971874683]\n",
            "Step 44950, Loss 0.7600857615470886\n",
            "Step 44975, Loss 0.8698923587799072\n",
            "Step 45000, Loss 0.6361404657363892\n",
            "Step 45025, Loss 0.8356926441192627\n",
            "Step 45050, Loss 0.8027780055999756\n",
            "Step 45075, Loss 0.6566085815429688\n",
            "Step 45100, Loss 0.7633763551712036\n",
            "Step 45125, Loss 0.8857131004333496\n",
            "Step 45150, Loss 0.6908013820648193\n",
            "Step 45175, Loss 0.8216006755828857\n",
            "Step 45200, Loss 0.8974743485450745\n",
            "Step 45225, Loss 0.7442325353622437\n",
            "Step 45250, Loss 1.0422108173370361\n",
            "Step 45275, Loss 0.8734526634216309\n",
            "Starting epoch 130/160, LR = [0.05782172325201103]\n",
            "Step 45300, Loss 0.8038349747657776\n",
            "Step 45325, Loss 0.6145767569541931\n",
            "Step 45350, Loss 0.720335066318512\n",
            "Step 45375, Loss 0.7923732399940491\n",
            "Step 45400, Loss 0.7683769464492798\n",
            "Step 45425, Loss 0.5974858999252319\n",
            "Step 45450, Loss 0.7543221712112427\n",
            "Step 45475, Loss 0.8711915612220764\n",
            "Step 45500, Loss 0.7578379511833191\n",
            "Step 45525, Loss 0.6927140951156616\n",
            "Step 45550, Loss 0.6763317584991455\n",
            "Step 45575, Loss 0.7353371381759644\n",
            "Step 45600, Loss 0.8060232400894165\n",
            "Step 45625, Loss 0.8554990291595459\n",
            "Starting epoch 131/160, LR = [0.04999999999999969]\n",
            "Step 45650, Loss 0.6988516449928284\n",
            "Step 45675, Loss 0.6323114633560181\n",
            "Step 45700, Loss 0.992189884185791\n",
            "Step 45725, Loss 0.8011823892593384\n",
            "Step 45750, Loss 0.8598201870918274\n",
            "Step 45775, Loss 0.7707917094230652\n",
            "Step 45800, Loss 0.5230112075805664\n",
            "Step 45825, Loss 0.7372868061065674\n",
            "Step 45850, Loss 0.6468949913978577\n",
            "Step 45875, Loss 0.6925660967826843\n",
            "Step 45900, Loss 0.6924744248390198\n",
            "Step 45925, Loss 0.5693048238754272\n",
            "Step 45950, Loss 0.6927151679992676\n",
            "Step 45975, Loss 0.7349897623062134\n",
            "Starting epoch 132/160, LR = [0.04217827674798799]\n",
            "Step 46000, Loss 0.6687924861907959\n",
            "Step 46025, Loss 0.5059624910354614\n",
            "Step 46050, Loss 0.6208767890930176\n",
            "Step 46075, Loss 0.76852947473526\n",
            "Step 46100, Loss 0.598686158657074\n",
            "Step 46125, Loss 0.6889493465423584\n",
            "Step 46150, Loss 0.6134119033813477\n",
            "Step 46175, Loss 0.5660592913627625\n",
            "Step 46200, Loss 0.6130251288414001\n",
            "Step 46225, Loss 0.5292755365371704\n",
            "Step 46250, Loss 0.7106159925460815\n",
            "Step 46275, Loss 0.5048943758010864\n",
            "Step 46300, Loss 0.7345269918441772\n",
            "Step 46325, Loss 0.5640435218811035\n",
            "Starting epoch 133/160, LR = [0.03454915028125236]\n",
            "Step 46350, Loss 0.6619172096252441\n",
            "Step 46375, Loss 0.6309515237808228\n",
            "Step 46400, Loss 0.6120918989181519\n",
            "Step 46425, Loss 0.6891498565673828\n",
            "Step 46450, Loss 0.5632010102272034\n",
            "Step 46475, Loss 0.588741660118103\n",
            "Step 46500, Loss 0.7396053075790405\n",
            "Step 46525, Loss 0.6377574801445007\n",
            "Step 46550, Loss 0.5666607022285461\n",
            "Step 46575, Loss 0.6412400007247925\n",
            "Step 46600, Loss 0.6169265508651733\n",
            "Step 46625, Loss 0.3609294295310974\n",
            "Step 46650, Loss 0.5905513167381287\n",
            "Step 46675, Loss 0.5301368832588196\n",
            "Starting epoch 134/160, LR = [0.027300475013022418]\n",
            "Step 46700, Loss 0.4859367907047272\n",
            "Step 46725, Loss 0.46156951785087585\n",
            "Step 46750, Loss 0.3967640697956085\n",
            "Step 46775, Loss 0.3754144310951233\n",
            "Step 46825, Loss 0.4580920338630676\n",
            "Step 46850, Loss 0.45043087005615234\n",
            "Step 46875, Loss 0.4611504077911377\n",
            "Step 46900, Loss 0.5053550601005554\n",
            "Step 46925, Loss 0.6679389476776123\n",
            "Step 46950, Loss 0.6698771715164185\n",
            "Step 46975, Loss 0.4707120656967163\n",
            "Step 47000, Loss 0.567561149597168\n",
            "Step 47025, Loss 0.5279560685157776\n",
            "Starting epoch 135/160, LR = [0.020610737385376266]\n",
            "Step 47050, Loss 0.3632727861404419\n",
            "Step 47075, Loss 0.47884058952331543\n",
            "Step 47100, Loss 0.4284178912639618\n",
            "Step 47125, Loss 0.3403766453266144\n",
            "Step 47150, Loss 0.3552002012729645\n",
            "Step 47175, Loss 0.4825805127620697\n",
            "Step 47200, Loss 0.3911433815956116\n",
            "Step 47225, Loss 0.563472330570221\n",
            "Step 47250, Loss 0.4249163269996643\n",
            "Step 47275, Loss 0.5023013353347778\n",
            "Step 47300, Loss 0.436047226190567\n",
            "Step 47325, Loss 0.5074177384376526\n",
            "Step 47350, Loss 0.3036256432533264\n",
            "Step 47375, Loss 0.5511583089828491\n",
            "Starting epoch 136/160, LR = [0.014644660940672554]\n",
            "Step 47400, Loss 0.35264110565185547\n",
            "Step 47425, Loss 0.3789379894733429\n",
            "Step 47450, Loss 0.4705788493156433\n",
            "Step 47475, Loss 0.4041467308998108\n",
            "Step 47500, Loss 0.3329958915710449\n",
            "Step 47525, Loss 0.3212505578994751\n",
            "Step 47550, Loss 0.3574851155281067\n",
            "Step 47575, Loss 0.4470660388469696\n",
            "Step 47600, Loss 0.5605247020721436\n",
            "Step 47625, Loss 0.4073807895183563\n",
            "Step 47650, Loss 0.3800652027130127\n",
            "Step 47675, Loss 0.29793161153793335\n",
            "Step 47700, Loss 0.3323477506637573\n",
            "Step 47725, Loss 0.5414543747901917\n",
            "Starting epoch 137/160, LR = [0.009549150281252571]\n",
            "Step 47750, Loss 0.28025761246681213\n",
            "Step 47775, Loss 0.336042195558548\n",
            "Step 47800, Loss 0.3289477825164795\n",
            "Step 47825, Loss 0.43360042572021484\n",
            "Step 47850, Loss 0.4173721969127655\n",
            "Step 47875, Loss 0.42357271909713745\n",
            "Step 47900, Loss 0.4072369635105133\n",
            "Step 47925, Loss 0.5039803981781006\n",
            "Step 47950, Loss 0.42261114716529846\n",
            "Step 47975, Loss 0.32792094349861145\n",
            "Step 48000, Loss 0.3810986578464508\n",
            "Step 48025, Loss 0.39733076095581055\n",
            "Step 48050, Loss 0.3632594645023346\n",
            "Step 48075, Loss 0.46234893798828125\n",
            "Starting epoch 138/160, LR = [0.005449673790581557]\n",
            "Step 48100, Loss 0.342438280582428\n",
            "Step 48125, Loss 0.30687829852104187\n",
            "Step 48150, Loss 0.3154789209365845\n",
            "Step 48175, Loss 0.3622339963912964\n",
            "Step 48200, Loss 0.2881509065628052\n",
            "Step 48225, Loss 0.3753716051578522\n",
            "Step 48250, Loss 0.2993762195110321\n",
            "Step 48275, Loss 0.4516499936580658\n",
            "Step 48300, Loss 0.4154065251350403\n",
            "Step 48325, Loss 0.30322179198265076\n",
            "Step 48350, Loss 0.3418923616409302\n",
            "Step 48375, Loss 0.44537439942359924\n",
            "Step 48400, Loss 0.3965540826320648\n",
            "Step 48425, Loss 0.35428622364997864\n",
            "Starting epoch 139/160, LR = [0.0024471741852422854]\n",
            "Step 48450, Loss 0.33491548895835876\n",
            "Step 48475, Loss 0.25012028217315674\n",
            "Step 48500, Loss 0.335843563079834\n",
            "Step 48525, Loss 0.27290868759155273\n",
            "Step 48550, Loss 0.36432796716690063\n",
            "Step 48575, Loss 0.318727046251297\n",
            "Step 48600, Loss 0.4170548617839813\n",
            "Step 48625, Loss 0.32697394490242004\n",
            "Step 48650, Loss 0.34469661116600037\n",
            "Step 48675, Loss 0.23780077695846558\n",
            "Step 48700, Loss 0.18049103021621704\n",
            "Step 48725, Loss 0.3061523735523224\n",
            "Step 48750, Loss 0.28772759437561035\n",
            "Step 48775, Loss 0.28946930170059204\n",
            "Starting epoch 140/160, LR = [0.0006155829702431228]\n",
            "Step 48800, Loss 0.3442589342594147\n",
            "Step 48825, Loss 0.3802788257598877\n",
            "Step 48850, Loss 0.2884420156478882\n",
            "Step 48875, Loss 0.2684217393398285\n",
            "Step 48900, Loss 0.3149140477180481\n",
            "Step 48925, Loss 0.36234354972839355\n",
            "Step 48950, Loss 0.31939536333084106\n",
            "Step 48975, Loss 0.21276023983955383\n",
            "Step 49000, Loss 0.20313365757465363\n",
            "Step 49025, Loss 0.3046901822090149\n",
            "Step 49050, Loss 0.2442651242017746\n",
            "Step 49075, Loss 0.34199637174606323\n",
            "Step 49100, Loss 0.33904194831848145\n",
            "Step 49125, Loss 0.262312650680542\n",
            "Starting epoch 141/160, LR = [0.0]\n",
            "Step 49150, Loss 0.3423720598220825\n",
            "Step 49175, Loss 0.3227544128894806\n",
            "Step 49200, Loss 0.33204877376556396\n",
            "Step 49225, Loss 0.2971431612968445\n",
            "Step 49250, Loss 0.4274764657020569\n",
            "Step 49275, Loss 0.27405670285224915\n",
            "Step 49300, Loss 0.26844653487205505\n",
            "Step 49325, Loss 0.39093902707099915\n",
            "Step 49350, Loss 0.34691840410232544\n",
            "Step 49375, Loss 0.2915645241737366\n",
            "Step 49400, Loss 0.3172766864299774\n",
            "Step 49425, Loss 0.38753047585487366\n",
            "Step 49450, Loss 0.3256588578224182\n",
            "Step 49475, Loss 0.46076837182044983\n",
            "Starting epoch 142/160, LR = [0.0006155829702431115]\n",
            "Step 49500, Loss 0.2897292375564575\n",
            "Step 49525, Loss 0.3271437883377075\n",
            "Step 49550, Loss 0.3721773624420166\n",
            "Step 49575, Loss 0.2271270751953125\n",
            "Step 49600, Loss 0.33259648084640503\n",
            "Step 49625, Loss 0.34680378437042236\n",
            "Step 49650, Loss 0.30816176533699036\n",
            "Step 49675, Loss 0.315434992313385\n",
            "Step 49700, Loss 0.255260169506073\n",
            "Step 49725, Loss 0.3481059670448303\n",
            "Step 49750, Loss 0.38608843088150024\n",
            "Step 49775, Loss 0.43317854404449463\n",
            "Step 49800, Loss 0.3375299572944641\n",
            "Step 49825, Loss 0.2429230511188507\n",
            "Starting epoch 143/160, LR = [0.002447174185242445]\n",
            "Step 49850, Loss 0.3173186480998993\n",
            "Step 49875, Loss 0.331064909696579\n",
            "Step 49900, Loss 0.29159510135650635\n",
            "Step 49925, Loss 0.3140033483505249\n",
            "Step 49950, Loss 0.26066717505455017\n",
            "Step 49975, Loss 0.31090784072875977\n",
            "Step 50000, Loss 0.33914104104042053\n",
            "Step 50025, Loss 0.3667767643928528\n",
            "Step 50050, Loss 0.3077050745487213\n",
            "Step 50075, Loss 0.22040477395057678\n",
            "Step 50100, Loss 0.32585614919662476\n",
            "Step 50125, Loss 0.39465150237083435\n",
            "Step 50150, Loss 0.27820199728012085\n",
            "Step 50175, Loss 0.3307873010635376\n",
            "Starting epoch 144/160, LR = [0.005449673790581812]\n",
            "Step 50200, Loss 0.2782764434814453\n",
            "Step 50225, Loss 0.3116709589958191\n",
            "Step 50250, Loss 0.4069061875343323\n",
            "Step 50275, Loss 0.28527992963790894\n",
            "Step 50300, Loss 0.420276015996933\n",
            "Step 50325, Loss 0.2442857027053833\n",
            "Step 50350, Loss 0.37276139855384827\n",
            "Step 50375, Loss 0.3539447486400604\n",
            "Step 50400, Loss 0.3362175524234772\n",
            "Step 50425, Loss 0.36935269832611084\n",
            "Step 50450, Loss 0.2519413232803345\n",
            "Step 50475, Loss 0.34007376432418823\n",
            "Step 50500, Loss 0.4141886830329895\n",
            "Step 50525, Loss 0.3585703372955322\n",
            "Starting epoch 145/160, LR = [0.009549150281253036]\n",
            "Step 50550, Loss 0.38761091232299805\n",
            "Step 50575, Loss 0.36699071526527405\n",
            "Step 50600, Loss 0.33959999680519104\n",
            "Step 50625, Loss 0.291702538728714\n",
            "Step 50650, Loss 0.32567092776298523\n",
            "Step 50675, Loss 0.3289353549480438\n",
            "Step 50700, Loss 0.2844054102897644\n",
            "Step 50725, Loss 0.3911830186843872\n",
            "Step 50750, Loss 0.3855287432670593\n",
            "Step 50775, Loss 0.4607272744178772\n",
            "Step 50800, Loss 0.3136679530143738\n",
            "Step 50825, Loss 0.3461828827857971\n",
            "Step 50850, Loss 0.296034038066864\n",
            "Step 50875, Loss 0.37973466515541077\n",
            "Starting epoch 146/160, LR = [0.014644660940673281]\n",
            "Step 50900, Loss 0.31113681197166443\n",
            "Step 50925, Loss 0.38442671298980713\n",
            "Step 50950, Loss 0.3779827058315277\n",
            "Step 50975, Loss 0.3800770044326782\n",
            "Step 51000, Loss 0.44116511940956116\n",
            "Step 51025, Loss 0.41330114006996155\n",
            "Step 51050, Loss 0.4173262417316437\n",
            "Step 51075, Loss 0.41100749373435974\n",
            "Step 51100, Loss 0.42548123002052307\n",
            "Step 51125, Loss 0.40307655930519104\n",
            "Step 51150, Loss 0.46829667687416077\n",
            "Step 51175, Loss 0.4636104106903076\n",
            "Step 51200, Loss 0.4029194712638855\n",
            "Step 51225, Loss 0.31707680225372314\n",
            "Starting epoch 147/160, LR = [0.020610737385377165]\n",
            "Step 51250, Loss 0.32825779914855957\n",
            "Step 51275, Loss 0.43356645107269287\n",
            "Step 51300, Loss 0.3532302975654602\n",
            "Step 51325, Loss 0.4828610420227051\n",
            "Step 51350, Loss 0.42603349685668945\n",
            "Step 51375, Loss 0.5167534947395325\n",
            "Step 51400, Loss 0.3377629518508911\n",
            "Step 51425, Loss 0.4646182656288147\n",
            "Step 51450, Loss 0.4200792908668518\n",
            "Step 51475, Loss 0.31783992052078247\n",
            "Step 51500, Loss 0.40711984038352966\n",
            "Step 51525, Loss 0.5514513850212097\n",
            "Step 51550, Loss 0.4035486578941345\n",
            "Step 51575, Loss 0.39696216583251953\n",
            "Starting epoch 148/160, LR = [0.02730047501302397]\n",
            "Step 51600, Loss 0.36645573377609253\n",
            "Step 51625, Loss 0.4038029909133911\n",
            "Step 51650, Loss 0.558319628238678\n",
            "Step 51675, Loss 0.3814798593521118\n",
            "Step 51700, Loss 0.4570061266422272\n",
            "Step 51725, Loss 0.5301769375801086\n",
            "Step 51750, Loss 0.30308324098587036\n",
            "Step 51775, Loss 0.6604982614517212\n",
            "Step 51800, Loss 0.47303926944732666\n",
            "Step 51825, Loss 0.5495471954345703\n",
            "Step 51850, Loss 0.36952751874923706\n",
            "Step 51875, Loss 0.5072344541549683\n",
            "Step 51900, Loss 0.5170503854751587\n",
            "Step 51925, Loss 0.5994585156440735\n",
            "Starting epoch 149/160, LR = [0.03454915028125413]\n",
            "Step 51950, Loss 0.32709038257598877\n",
            "Step 51975, Loss 0.49840986728668213\n",
            "Step 52000, Loss 0.4790613651275635\n",
            "Step 52025, Loss 0.588172197341919\n",
            "Step 52050, Loss 0.4665291905403137\n",
            "Step 52075, Loss 0.5859399437904358\n",
            "Step 52100, Loss 0.6045694947242737\n",
            "Step 52125, Loss 0.5635082125663757\n",
            "Step 52150, Loss 0.4880710244178772\n",
            "Step 52175, Loss 0.5698946714401245\n",
            "Step 52200, Loss 0.6596488356590271\n",
            "Step 52225, Loss 0.6105169057846069\n",
            "Step 52250, Loss 0.4936673939228058\n",
            "Step 52275, Loss 0.6091616153717041\n",
            "Starting epoch 150/160, LR = [0.042178276747990344]\n",
            "Step 52300, Loss 0.5014166831970215\n",
            "Step 52325, Loss 0.43546628952026367\n",
            "Step 52350, Loss 0.46491539478302\n",
            "Step 52375, Loss 0.5117086172103882\n",
            "Step 52400, Loss 0.5666214823722839\n",
            "Step 52425, Loss 0.7204074859619141\n",
            "Step 52450, Loss 0.5701751708984375\n",
            "Step 52475, Loss 0.6614329814910889\n",
            "Step 52500, Loss 0.7674590945243835\n",
            "Step 52525, Loss 0.8435627818107605\n",
            "Step 52550, Loss 0.7260539531707764\n",
            "Step 52575, Loss 0.6559883952140808\n",
            "Step 52600, Loss 0.6141214966773987\n",
            "Step 52625, Loss 0.684917688369751\n",
            "Starting epoch 151/160, LR = [0.05000000000000211]\n",
            "Step 52650, Loss 0.7704401016235352\n",
            "Step 52675, Loss 0.6705104112625122\n",
            "Step 52700, Loss 0.4100624620914459\n",
            "Step 52725, Loss 0.5798255205154419\n",
            "Step 52750, Loss 0.8129759430885315\n",
            "Step 52775, Loss 0.9406640529632568\n",
            "Step 52800, Loss 1.0072035789489746\n",
            "Step 52825, Loss 0.8121440410614014\n",
            "Step 52850, Loss 0.8205280900001526\n",
            "Step 52875, Loss 0.7996881604194641\n",
            "Step 52900, Loss 0.6725381016731262\n",
            "Step 52925, Loss 0.6602574586868286\n",
            "Step 52950, Loss 0.777691125869751\n",
            "Step 52975, Loss 0.7814618349075317\n",
            "Step 53000, Loss 0.8257859349250793\n",
            "Starting epoch 152/160, LR = [0.05782172325201405]\n",
            "Step 53025, Loss 0.8137659430503845\n",
            "Step 53050, Loss 0.8030892014503479\n",
            "Step 53075, Loss 0.882398784160614\n",
            "Step 53100, Loss 0.8420615792274475\n",
            "Step 53125, Loss 0.852349579334259\n",
            "Step 53150, Loss 1.0076453685760498\n",
            "Step 53175, Loss 0.7087680101394653\n",
            "Step 53200, Loss 0.8561075329780579\n",
            "Step 53225, Loss 0.9685884118080139\n",
            "Step 53250, Loss 0.8585736751556396\n",
            "Step 53275, Loss 0.7865041494369507\n",
            "Step 53300, Loss 0.8709214925765991\n",
            "Step 53325, Loss 0.8647509813308716\n",
            "Step 53350, Loss 0.8479176163673401\n",
            "Starting epoch 153/160, LR = [0.06545084971875027]\n",
            "Step 53375, Loss 0.8784976005554199\n",
            "Step 53400, Loss 0.6373465061187744\n",
            "Step 53425, Loss 0.7892142534255981\n",
            "Step 53450, Loss 0.7123898267745972\n",
            "Step 53475, Loss 1.0167304277420044\n",
            "Step 53500, Loss 0.6727081537246704\n",
            "Step 53525, Loss 0.9569958448410034\n",
            "Step 53550, Loss 0.8817687034606934\n",
            "Step 53575, Loss 1.0993554592132568\n",
            "Step 53600, Loss 0.7008756399154663\n",
            "Step 53625, Loss 0.8530429005622864\n",
            "Step 53650, Loss 0.9265902042388916\n",
            "Step 53675, Loss 0.9384502172470093\n",
            "Step 53700, Loss 1.0755521059036255\n",
            "Starting epoch 154/160, LR = [0.07269952498698061]\n",
            "Step 53725, Loss 0.7516942024230957\n",
            "Step 53750, Loss 0.9084442257881165\n",
            "Step 53775, Loss 0.6982285380363464\n",
            "Step 53800, Loss 0.864107608795166\n",
            "Step 53825, Loss 1.0247882604599\n",
            "Step 53850, Loss 0.6231691837310791\n",
            "Step 53875, Loss 1.0889531373977661\n",
            "Step 53900, Loss 0.9508728981018066\n",
            "Step 53925, Loss 0.8679453134536743\n",
            "Step 53950, Loss 0.8973005414009094\n",
            "Step 53975, Loss 0.7735428810119629\n",
            "Step 54000, Loss 1.2733911275863647\n",
            "Step 54025, Loss 1.0050328969955444\n",
            "Step 54050, Loss 0.740828812122345\n",
            "Starting epoch 155/160, LR = [0.07938926261462727]\n",
            "Step 54075, Loss 0.9849511384963989\n",
            "Step 54100, Loss 1.0657943487167358\n",
            "Step 54125, Loss 1.0082887411117554\n",
            "Step 54150, Loss 0.9481538534164429\n",
            "Step 54175, Loss 0.8709847927093506\n",
            "Step 54200, Loss 1.1145302057266235\n",
            "Step 54225, Loss 0.9191384315490723\n",
            "Step 54250, Loss 1.0093618631362915\n",
            "Step 54275, Loss 1.0932422876358032\n",
            "Step 54300, Loss 1.1121814250946045\n",
            "Step 54325, Loss 0.8137539625167847\n",
            "Step 54350, Loss 0.9973800182342529\n",
            "Step 54375, Loss 0.8865033984184265\n",
            "Step 54400, Loss 0.9092076420783997\n",
            "Starting epoch 156/160, LR = [0.08535533905933115]\n",
            "Step 54425, Loss 1.0532200336456299\n",
            "Step 54450, Loss 1.2801162004470825\n",
            "Step 54475, Loss 0.7692759037017822\n",
            "Step 54500, Loss 1.0182204246520996\n",
            "Step 54525, Loss 1.040817141532898\n",
            "Step 54550, Loss 1.0511250495910645\n",
            "Step 54575, Loss 0.8315962553024292\n",
            "Step 54600, Loss 0.9214853048324585\n",
            "Step 54625, Loss 1.0379743576049805\n",
            "Step 54650, Loss 0.7717252969741821\n",
            "Step 54675, Loss 0.9371902942657471\n",
            "Step 54700, Loss 1.087343692779541\n",
            "Step 54725, Loss 1.030322790145874\n",
            "Step 54750, Loss 1.2179226875305176\n",
            "Starting epoch 157/160, LR = [0.09045084971875143]\n",
            "Step 54775, Loss 0.7765359878540039\n",
            "Step 54800, Loss 0.7613207101821899\n",
            "Step 54825, Loss 0.8038069605827332\n",
            "Step 54850, Loss 0.974443793296814\n",
            "Step 54875, Loss 1.0611608028411865\n",
            "Step 54900, Loss 1.1236588954925537\n",
            "Step 54925, Loss 0.8955297470092773\n",
            "Step 54950, Loss 0.9717753529548645\n",
            "Step 54975, Loss 0.9705957770347595\n",
            "Step 55000, Loss 0.9653571248054504\n",
            "Step 55025, Loss 0.8852099180221558\n",
            "Step 55050, Loss 0.9805589318275452\n",
            "Step 55075, Loss 1.016353726387024\n",
            "Step 55100, Loss 0.9991714358329773\n",
            "Starting epoch 158/160, LR = [0.09455032620942257]\n",
            "Step 55125, Loss 0.7965445518493652\n",
            "Step 55150, Loss 1.0897825956344604\n",
            "Step 55175, Loss 0.7958051562309265\n",
            "Step 55200, Loss 0.9480124115943909\n",
            "Step 55225, Loss 1.0479681491851807\n",
            "Step 55250, Loss 1.0370652675628662\n",
            "Step 55275, Loss 0.9967196583747864\n",
            "Step 55300, Loss 1.076012134552002\n",
            "Step 55325, Loss 0.95250004529953\n",
            "Step 55350, Loss 0.9466311931610107\n",
            "Step 55375, Loss 1.1121547222137451\n",
            "Step 55400, Loss 1.041678786277771\n",
            "Step 55425, Loss 1.079217791557312\n",
            "Step 55450, Loss 0.9867457747459412\n",
            "Starting epoch 159/160, LR = [0.09755282581476207]\n",
            "Step 55475, Loss 1.0990746021270752\n",
            "Step 55500, Loss 0.8865575194358826\n",
            "Step 55525, Loss 1.1005955934524536\n",
            "Step 55550, Loss 0.7970966696739197\n",
            "Step 55575, Loss 0.9372535943984985\n",
            "Step 55600, Loss 1.3811079263687134\n",
            "Step 55625, Loss 1.067767858505249\n",
            "Step 55650, Loss 1.1567773818969727\n",
            "Step 55675, Loss 1.260038137435913\n",
            "Step 55700, Loss 0.9738686084747314\n",
            "Step 55725, Loss 1.3057606220245361\n",
            "Step 55750, Loss 0.8091914653778076\n",
            "Step 55775, Loss 1.029686689376831\n",
            "Step 55800, Loss 1.107530117034912\n",
            "Starting epoch 160/160, LR = [0.09938441702976132]\n",
            "Step 55825, Loss 0.9744548797607422\n",
            "Step 55850, Loss 1.2101993560791016\n",
            "Step 55875, Loss 1.3905200958251953\n",
            "Step 55900, Loss 0.8434993028640747\n",
            "Step 55925, Loss 1.0123271942138672\n",
            "Step 55950, Loss 0.9644600749015808\n",
            "Step 55975, Loss 0.8026665449142456\n",
            "Step 56000, Loss 0.9578980803489685\n",
            "Step 56025, Loss 0.9734990000724792\n",
            "Step 56050, Loss 1.0594817399978638\n",
            "Step 56075, Loss 0.9294564723968506\n",
            "Step 56100, Loss 0.9628982543945312\n",
            "Step 56125, Loss 1.2746245861053467\n",
            "Step 56150, Loss 1.2460988759994507\n"
          ]
        }
      ],
      "source": [
        "# By default, everything is loaded to cpu\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "current_step = 0\n",
        "\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "train_losses = []\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in trainloader:\n",
        "    aug_images = []\n",
        "\n",
        "    for image in images:\n",
        "      aug_image = aug_pipeline(image) \n",
        "      aug_images.append(aug_image) \n",
        "\n",
        "    aug_images = torch.stack(aug_images)\n",
        "\n",
        "    # Bring data over the device of choice\n",
        "    aug_images = aug_images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    net.train() # Sets module in training mode\n",
        "\n",
        "    optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = net(aug_images)\n",
        "\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Log loss\n",
        "    if current_step % LOG_FREQUENCY == 0:\n",
        "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "    current_step += 1\n",
        "\n",
        "  train_acc, train_loss = evaluate(net, trainloader, print_tqdm = False)\n",
        "  train_accuracies.append(train_acc)\n",
        "  train_losses.append(train_loss)\n",
        "  \n",
        "  val_acc, val_loss = evaluate(net, validloader, print_tqdm = False)\n",
        "  val_accuracies.append(val_acc)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  # Step the scheduler\n",
        "  scheduler.step() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "BhdMME-rHSo6",
        "outputId": "3edd6ed6-8697-4d96-e947-b0c6fee3ae0a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-04b500ecfc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best accuracies on the validation set: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_acc' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Best accuracies on the validation set: \", val_acc)\n",
        "\n",
        "results_df = pd.DataFrame(zip(range(1,NUM_EPOCHS+1), train_accuracies, val_accuracies, train_losses, val_losses), columns = [\"epoch\", \"train_accuracy\", \"val_accuracy\", \"train_loss\", \"val_loss\"])\n",
        "results_df.set_index(\"epoch\")\n",
        "results_df.plot(x = \"epoch\", y = [\"train_accuracy\", \"val_accuracy\"])\n",
        "results_df.plot(x = \"epoch\", y = [\"train_loss\", \"val_loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSHcUqLB5yWO",
        "outputId": "ac7905d8-1b9e-4935-a41a-3b4278720898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 78/78 [00:02<00:00, 35.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.5796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "accuracy = evaluate(net, testloader)[0]\n",
        "print('\\nTest Accuracy: {}'.format(accuracy))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3430fd8b8f3647ee8c90a217b2b0dc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6dcd503460e49cba8eb1304ab1c71c0",
              "IPY_MODEL_61e57d90bd1a4c2d9181eabf74406c00",
              "IPY_MODEL_ec3bc76d5f1949c6a4a6f7ef051925ce"
            ],
            "layout": "IPY_MODEL_27e83afbd28145238db1781af8671328"
          }
        },
        "c6dcd503460e49cba8eb1304ab1c71c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a4939aa420a4166b65fb9548d90fe30",
            "placeholder": "​",
            "style": "IPY_MODEL_1fd734c10c4e4de0858e6332d2426e04",
            "value": ""
          }
        },
        "61e57d90bd1a4c2d9181eabf74406c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49c8d9f40565447097e5c93f043e3cad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b93241990a674a389489c5deddfcf77f",
            "value": 1
          }
        },
        "ec3bc76d5f1949c6a4a6f7ef051925ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ede10f96f5540849e8a268d94c273b3",
            "placeholder": "​",
            "style": "IPY_MODEL_e1ff9c5deca94aa199ba4528892cf84f",
            "value": " 169009152/? [04:44&lt;00:00, 13244491.33it/s]"
          }
        },
        "27e83afbd28145238db1781af8671328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a4939aa420a4166b65fb9548d90fe30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd734c10c4e4de0858e6332d2426e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49c8d9f40565447097e5c93f043e3cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b93241990a674a389489c5deddfcf77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ede10f96f5540849e8a268d94c273b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1ff9c5deca94aa199ba4528892cf84f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}